\relax 
\citation{trabelsi2021pose,hoang2024object,wang2021gdr}
\citation{hoang2024graspability,peng2019pvnet,wang2021gdr}
\citation{chao2021dexycb,hoang2024multi,garcia2018first,llop2022benchmarking}
\citation{wang20216d,li2019cdpn,hoang2020panoptic}
\citation{billings2019silhonet,peng2019pvnet}
\citation{trabelsi2021pose,rad2017bb8,hoang2020object}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\newlabel{sec:intro}{{I}{1}}
\citation{doosti2020hope,lin2023harmonious,wang2023interacting}
\citation{romero2022embodied}
\citation{wang20216d,gao20206d,guo2021efficient,hoang2022voting}
\citation{billings2019silhonet,peng2019pvnet,wang2021gdr}
\citation{wang2019densefusion,he2020pvn3d,hong2024rdpn6d}
\citation{xiang2017posecnn,kehl2017ssd,trabelsi2021pose,wang2021gdr}
\citation{rad2017bb8,tekin2018real,oberweger2018making,peng2019pvnet}
\citation{li2019cdpn,park2019pix2pose,zakharov2019dpod}
\citation{xiang2017posecnn,kehl2017ssd,trabelsi2021pose}
\citation{hu2020single,chen2020end,wang2021gdr}
\citation{lepetit2009ep}
\citation{rad2017bb8,tekin2018real,oberweger2018making,peng2019pvnet}
\citation{lepetit2009ep}
\citation{li2019cdpn,park2019pix2pose,zakharov2019dpod}
\citation{sun2021robust,huang2021pixel,stoiber2020sparse,tian2022large}
\citation{garon2017deep,marougkas2020track,manhardt2018deep,li2018deepim,dosovitskiy2015flownet,wen2020se}
\citation{deng2021poserbpf,majcher20203d,zhong2020seeing,wang2023deep}
\citation{garon2017deep}
\citation{marougkas2020track}
\citation{manhardt2018deep}
\citation{li2018deepim}
\citation{dosovitskiy2015flownet}
\citation{wen2020se}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work}{3}}
\newlabel{sec:Related}{{II}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-A}}Object Pose Estimation from a Single Image}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-B}}Deep Learning-based Object Pose Tracking}{3}}
\citation{deng2021poserbpf}
\citation{majcher20203d}
\citation{zhong2020seeing}
\citation{wang2023deep}
\citation{vaswani2017attention}
\citation{zhou2023deep,hong2024transformer,li2023depth,periyasamy2023yolopose}
\citation{zhou2023deep}
\citation{hong2024transformer}
\citation{zhou2023deep,hong2024transformer}
\citation{li2023depth}
\citation{liu2021swin}
\citation{gao2023visfusion}
\citation{huang2025visibility,chen2020visibility}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-C}}Transformer-based Methods}{4}}
\citation{carion2020end}
\citation{lin2017feature}
\citation{he2016deep}
\citation{vaswani2017attention}
\citation{oberweger2018making,tekin2018real,rad2017bb8}
\citation{jaegle2021perceiver}
\citation{vaswani2017attention}
\@writefile{toc}{\contentsline {section}{\numberline {III}Method}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Spatial-Temporal Transformer}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-A}1}CNN Backbone}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-A}2}Spatial Transformer}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-A}3}Temporal Transformer}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Overview of the proposed framework for in-hand object pose tracking. Our approach leverages transformers to capture both spatial relationships within individual frames and temporal dependencies across image sequences, enabling robust frame-wise object pose estimation. To address challenges posed by heavy occlusion or motion blur, a visibility-aware module integrates temporally-aware features and aggregates pose information from neighboring frames. This allows the model to maintain accurate pose estimates even in difficult scenarios where direct visual cues are limited or absent.\relax }}{6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:overview}{{1}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}Visibility-Aware Object Pose Estimation Under Occlusions}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-B}1}Visibility Estimation}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-B}2}Object Pose Prediction Under Heavy Occlusion}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-C}}Loss Function}{7}}
\citation{chao2021dexycb}
\citation{garcia2018first}
\citation{hampali2020honnotate}
\citation{chao2021dexycb}
\citation{chao2021dexycb}
\citation{garcia2018first}
\citation{hampali2020honnotate}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Evaluation}{8}}
\newlabel{sec:Evaluation}{{IV}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Illustration of the ground truth visibility score computation process. (a) The input RGB image. (b) The projected 2D object mask overlaid with the hand mask, showing the occluded regions caused by the hand. (c) The projected 2D object mask without considering the hand, representing the total object pixels. The visibility score is computed as the ratio of visible object pixels in (b) to the total object pixels in (c).\relax }}{8}}
\newlabel{fig:compute_visibility_score}{{2}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Datasets}{8}}
\citation{he2016deep}
\citation{he2017mask}
\citation{wang2023deep}
\citation{wang2023deep}
\citation{wang2023deep}
\citation{wang2023deep}
\citation{wang2023deep}
\citation{wang2023deep}
\citation{wang2023deep}
\citation{wang2023deep}
\citation{hinterstoisser2012model}
\citation{bregier2017symmetry}
\citation{bregier2017symmetry}
\citation{wang2019densefusion}
\citation{wang2023deep}
\citation{chao2021dexycb}
\citation{garcia2018first}
\citation{hampali2020honnotate}
\citation{billings2019silhonet,peng2019pvnet,wang2021gdr,castro2023crt}
\citation{sun2021robust,huang2021pixel,he2021ffb6d,stoiber2020sparse,stoiber2022srt3d,tian2022large,wang2023deep}
\citation{chao2021dexycb}
\citation{garcia2018first}
\citation{hampali2020honnotate}
\citation{billings2019silhonet,peng2019pvnet,wang2021gdr,castro2023crt}
\citation{sun2021robust,huang2021pixel,he2021ffb6d,stoiber2020sparse,stoiber2022srt3d,tian2022large,wang2023deep}
\citation{billings2019silhonet}
\citation{peng2019pvnet}
\citation{wang2021gdr}
\citation{castro2023crt}
\citation{sun2021robust}
\citation{huang2021pixel}
\citation{stoiber2020sparse}
\citation{stoiber2022srt3d}
\citation{tian2022large}
\citation{wang2023deep}
\citation{chao2021dexycb}
\citation{garcia2018first}
\citation{hampali2020honnotate}
\citation{billings2019silhonet}
\citation{peng2019pvnet}
\citation{wang2021gdr}
\citation{castro2023crt}
\citation{wang2023deep}
\citation{castro2023crt}
\citation{wang2021gdr}
\citation{chao2021dexycb}
\citation{garcia2018first}
\citation{hampali2020honnotate}
\citation{billings2019silhonet,peng2019pvnet,wang2021gdr,castro2023crt}
\citation{sun2021robust,huang2021pixel,he2021ffb6d,stoiber2020sparse,stoiber2022srt3d,tian2022large,wang2023deep}
\citation{chao2021dexycb}
\citation{garcia2018first}
\citation{hampali2020honnotate}
\citation{billings2019silhonet,peng2019pvnet,wang2021gdr,castro2023crt}
\citation{sun2021robust,huang2021pixel,he2021ffb6d,stoiber2020sparse,stoiber2022srt3d,tian2022large,wang2023deep}
\citation{billings2019silhonet}
\citation{peng2019pvnet}
\citation{wang2021gdr}
\citation{castro2023crt}
\citation{sun2021robust}
\citation{huang2021pixel}
\citation{stoiber2020sparse}
\citation{stoiber2022srt3d}
\citation{tian2022large}
\citation{wang2023deep}
\citation{wang2023deep}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Implementation Details}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-C}}Evaluation Metric}{9}}
\newlabel{sec:metric}{{\unhbox \voidb@x \hbox {IV-C}}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-D}}Result}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Qualitative Comparison of the state-of-the-art video-based method \cite  {wang2023deep} and our method. The proposed method robustly generates more stable and accurate poses over time than previous approaches. (a) are the input RGB images. (b-c) shows the rendered images using predicted object poses and colorcoded visualization of point-wise distance error.\relax }}{10}}
\newlabel{fig:result1}{{3}{10}}
\citation{zhou2023deep}
\citation{zhou2023deep}
\citation{labbe2020cosypose}
\citation{hong2024transformer}
\citation{hong2024transformer}
\citation{labbe2020cosypose}
\citation{li2023depth}
\citation{li2023depth}
\citation{labbe2020cosypose}
\citation{periyasamy2023yolopose}
\citation{periyasamy2023yolopose}
\citation{labbe2020cosypose}
\citation{labbe2020cosypose}
\citation{zhou2023deep}
\citation{hong2024transformer}
\citation{li2023depth}
\citation{periyasamy2023yolopose}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Qualitative Comparison of the state-of-the-art video-based method \cite  {wang2023deep}, and our method. Here we only show results on frames under heavy occlusion (frames with a visibility score \( v_i \) smaller than \( \delta = 0.5 \)). (a) are the input RGB images. (b-c) shows the rendered images using predicted object poses and colorcoded visualization of point-wise distance error.\relax }}{11}}
\newlabel{fig:result2}{{4}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-E}}Comparison with Transformer-Based Methods}{11}}
\citation{labbe2020cosypose}
\citation{zhou2023deep}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Quantitative results on the DexYCB \cite  {chao2021dexycb}, FPHAB \cite  {garcia2018first}, and HO-3D \cite  {hampali2020honnotate} datasets. Single image -based methods \cite  {billings2019silhonet, peng2019pvnet, wang2021gdr, castro2023crt}, and tracking methods \cite  {sun2021robust, huang2021pixel, he2021ffb6d, stoiber2020sparse, stoiber2022srt3d, tian2022large, wang2023deep} are compared with our proposed method (Ours).\relax }}{12}}
\newlabel{tab:dataset_ex}{{1}{12}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Quantitative results on the DexYCB \cite  {chao2021dexycb}, FPHAB \cite  {garcia2018first}, and HO-3D \cite  {hampali2020honnotate} datasets. However, here we only evaluate methods on frames under heavy occlusion (frames with a visibility score \( v_i \) smaller than \( \delta = 0.5 \)). Single image-based methods \cite  {billings2019silhonet, peng2019pvnet, wang2021gdr, castro2023crt}, and tracking methods \cite  {sun2021robust, huang2021pixel, he2021ffb6d, stoiber2020sparse, stoiber2022srt3d, tian2022large, wang2023deep} are compared with our proposed method (Ours).\relax }}{12}}
\newlabel{tab:occlusion_ex}{{2}{12}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Performance comparison of our method with existing transformer-based approaches for 6D object pose estimation across three benchmark datasets. We only evaluate methods on frames under heavy occlusion (frames with a visibility score \( v_i \) smaller than \( \delta = 0.5 \)).\relax }}{12}}
\newlabel{tab:Transformer-Based}{{3}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-F}}Ablation Study}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-G}}Runtime Analysis and Computational Efficiency}{13}}
\bibstyle{IEEEtran}
\bibdata{References}
\bibcite{trabelsi2021pose}{1}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Ablation Study. $\mathcal  {M}_{s}$ spatial transformer, $\mathcal  {M}_{t}$ temporal transformer, $\mathcal  {M}_{va}$ visibility-aware object pose estimation module.\relax }}{14}}
\newlabel{tab:ablation_ex}{{4}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-H}}Limitations and Future Work}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Conclusion}{14}}
\bibcite{hoang2024object}{2}
\bibcite{wang2021gdr}{3}
\bibcite{hoang2024graspability}{4}
\bibcite{peng2019pvnet}{5}
\bibcite{chao2021dexycb}{6}
\bibcite{hoang2024multi}{7}
\bibcite{garcia2018first}{8}
\bibcite{llop2022benchmarking}{9}
\bibcite{wang20216d}{10}
\bibcite{li2019cdpn}{11}
\bibcite{hoang2020panoptic}{12}
\bibcite{billings2019silhonet}{13}
\bibcite{rad2017bb8}{14}
\bibcite{hoang2020object}{15}
\bibcite{doosti2020hope}{16}
\bibcite{lin2023harmonious}{17}
\bibcite{wang2023interacting}{18}
\bibcite{romero2022embodied}{19}
\bibcite{gao20206d}{20}
\bibcite{guo2021efficient}{21}
\bibcite{hoang2022voting}{22}
\bibcite{wang2019densefusion}{23}
\bibcite{he2020pvn3d}{24}
\bibcite{hong2024rdpn6d}{25}
\bibcite{xiang2017posecnn}{26}
\bibcite{kehl2017ssd}{27}
\bibcite{tekin2018real}{28}
\bibcite{oberweger2018making}{29}
\bibcite{park2019pix2pose}{30}
\bibcite{zakharov2019dpod}{31}
\bibcite{hu2020single}{32}
\bibcite{chen2020end}{33}
\bibcite{lepetit2009ep}{34}
\bibcite{sun2021robust}{35}
\bibcite{huang2021pixel}{36}
\bibcite{stoiber2020sparse}{37}
\bibcite{tian2022large}{38}
\bibcite{garon2017deep}{39}
\bibcite{marougkas2020track}{40}
\bibcite{manhardt2018deep}{41}
\bibcite{li2018deepim}{42}
\bibcite{dosovitskiy2015flownet}{43}
\bibcite{wen2020se}{44}
\@writefile{toc}{\contentsline {section}{REFERENCES}{15}}
\bibcite{deng2021poserbpf}{45}
\bibcite{majcher20203d}{46}
\bibcite{zhong2020seeing}{47}
\bibcite{wang2023deep}{48}
\bibcite{vaswani2017attention}{49}
\bibcite{zhou2023deep}{50}
\bibcite{hong2024transformer}{51}
\bibcite{li2023depth}{52}
\bibcite{periyasamy2023yolopose}{53}
\bibcite{liu2021swin}{54}
\bibcite{gao2023visfusion}{55}
\bibcite{huang2025visibility}{56}
\bibcite{chen2020visibility}{57}
\bibcite{carion2020end}{58}
\bibcite{lin2017feature}{59}
\bibcite{he2016deep}{60}
\bibcite{jaegle2021perceiver}{61}
\bibcite{hampali2020honnotate}{62}
\bibcite{he2017mask}{63}
\bibcite{hinterstoisser2012model}{64}
\bibcite{bregier2017symmetry}{65}
\bibcite{castro2023crt}{66}
\bibcite{he2021ffb6d}{67}
\bibcite{stoiber2022srt3d}{68}
\bibcite{labbe2020cosypose}{69}
\@writefile{toc}{\contentsline {subsection}{Phan Xuan Tan}{16}}
\@writefile{toc}{\contentsline {subsection}{Dinh-Cuong Hoang}{16}}
\@writefile{toc}{\contentsline {subsection}{Eiji Kamioka}{16}}
\@writefile{toc}{\contentsline {subsection}{Anh-Nhat Nguyen}{16}}
\@writefile{toc}{\contentsline {subsection}{Duc-Thanh Tran}{17}}
\@writefile{toc}{\contentsline {subsection}{Van-Hiep Duong}{17}}
\@writefile{toc}{\contentsline {subsection}{Anh-Truong Mai}{17}}
\@writefile{toc}{\contentsline {subsection}{Duc-Long Pham}{17}}
\@writefile{toc}{\contentsline {subsection}{Khanh-Toan Phan}{17}}
\@writefile{toc}{\contentsline {subsection}{Van-Hiep Duong}{17}}
\@writefile{toc}{\contentsline {subsection}{Xuan-Tung Dinh}{17}}
\@writefile{toc}{\contentsline {subsection}{TranThiThuyTrang}{17}}
\@writefile{toc}{\contentsline {subsection}{Xuan-Duong Pham}{17}}
\@writefile{toc}{\contentsline {subsection}{Nhat-Linh Nguyen}{17}}
\@writefile{toc}{\contentsline {subsection}{Thu-Uyen Nguyen}{18}}
\@writefile{toc}{\contentsline {subsection}{Viet-Anh Trinh}{18}}
\@writefile{toc}{\contentsline {subsection}{Khanh-Duong Tran}{18}}
\@writefile{toc}{\contentsline {subsection}{Son-Anh Bui}{18}}
