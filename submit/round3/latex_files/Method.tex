\section{Method}

\begin{figure*}[h!]
	\centering
	\includegraphics[width=0.98\linewidth]{figs/overview}
	\caption{Overview of the proposed framework for in-hand object pose tracking. Our approach leverages transformers to capture both spatial relationships within individual frames and temporal dependencies across image sequences, enabling robust frame-wise object pose estimation. To address challenges posed by heavy occlusion or motion blur, a visibility-aware module integrates temporally-aware features and aggregates pose information from neighboring frames. This allows the model to maintain accurate pose estimates even in difficult scenarios where direct visual cues are limited or absent.}
	\label{fig:overview}
\end{figure*}

The overview structure of our framework is illustrated in Figure \ref{fig:overview}. Given a sequence of RGB images $\mathcal{V} = \lbrace I_t \rbrace_{t=1}^{T}$, where $T$ represents the total number of frames, the objective is to estimate the object's 6D pose $\mathbf{P}_t = (\mathbf{R}_t, \mathbf{t}_t)$ for each frame $I_t$. This pose consists of the rotation matrix $\mathbf{R}_t$ and the translation vector $\mathbf{t}_t$, which describe the rigid transformation of the object from its coordinate system to the camera coordinate system. We assume that a 3D model of the object is available, and its coordinate system is defined within the 3D space of the model. The framework first utilizes a CNN backbone to extract spatial features \( \mathcal{F}_t \) from each frame. These features are then enhanced by a spatial transformer $\mathcal{M}_{s}$, which captures non-local dependencies within each frame to improve the modeling of complex hand-object interactions. The spatial transformer encoder produces contextually enriched features \( \mathcal{F}^s_t \), which are further processed by a temporal transformer $\mathcal{M}_{s}$ that models temporal relationships across frames. The temporal transformer outputs latent vectors \( \mathcal{F}^t_t \), which are passed through an MLP to predict the object's pose \( \hat{\mathbf{P}}_t \) for each frame. Additionally, a visibility-aware module $\mathcal{M}_{va}$ predicts the object's visibility score \( v_t \) for each frame, dynamically adjusting pose predictions based on occlusion levels. This module aggregates pose information from visible frames to support pose estimation in occluded frames using a cross-attention mechanism, ensuring robust performance under heavy occlusions and rapid motion.

\subsection{Spatial-Temporal Transformer}

\subsubsection{CNN Backbone}

Inspired by the hybrid model proposed by \cite{carion2020end}, our method begins by leveraging a truncated FPN \cite{lin2017feature} with ResNet50 \cite{he2016deep} to extract the initial in-hand object features $\mathcal{F}_t \in \mathbb{R}^{H \times W \times C}$ for each frame $I_t$. While CNNs, through their convolutional kernels, are effective at capturing local two-dimensional structures, they struggle with occlusions where local details are obscured or distorted. To mitigate this, we incorporate a transformer \cite{vaswani2017attention}, which is adept at handling non-local relationships and dependencies. The feature maps $\mathcal{F}_t$ are subsequently processed by a spatial transformer, which captures non-local dependencies within each frame, enhancing the network's ability to model complex interactions between the object and the hand.

\subsubsection{Spatial Transformer}

The spatial transformer encoder processes a sequence as input. To facilitate this, we flatten the spatial dimensions of the hand feature, resulting in $HW$ vectors, each with length $C$. These vectors, referred to as tokens, are then fed through multiple layers of multi-head self-attention and feed-forward networks (FFN). Since each transformer layer is permutation-invariant, we incorporate 2D positional embeddings into each token to provide spatial location information. The output feature $\mathcal{F}^s_t \in \mathbb{R}^{H \times W \times C}$ from the transformer encoder enhances the original feature $\mathcal{F}_t$ by capturing non-local interactions among all tokens. This enhanced feature $\mathcal{F}^s_t$ is expected to contain rich contextual information about the in-hand object, including the locations of object keypoints. We follow previous works \cite{oberweger2018making, tekin2018real, rad2017bb8} use the eight corners of the 3D bounding box of the object as the keypoints. As an intermediate supervision step, we regress a heatmap of object keypoint joints $\hat{\mathcal{H}}_t$ from $\mathcal{F}^s_t$, which provides the predicted locations of $N_j = 8$ keypoints as $\hat{\mathcal{K}} \in \mathbb{R}^{8 \times 2}$.

The spatial transformer decoder takes a learnable query vector $\mathbf{Q} \in \mathbb{R}^C$ as input and progressively integrates information from the combined features of $\mathcal{F}^s_t$ and $\hat{\mathcal{K}}$ through several layers of cross-attention and FFN. Unlike self-attention, which generates query, key, and value from the same set of tokens, cross-attention uses the learnable query vector to create the query and leverages the learned tokens for key and value. Following  \cite{jaegle2021perceiver}, the decoder adaptively selects relevant information from the feature map, producing a compact latent representation $\mathcal{F}^{se}_t \in \mathbb{R}^C$ of the object. This significantly reduces the computational cost for temporal reasoning in subsequent stages.

\subsubsection{Temporal Transformer}

Relying solely on single images for hand-held object pose estimation is often insufficient due to several challenges. One issue is the variability in the object's visual appearance over time, which can lead to inconsistent predictions from image-based models. Additionally, the object is frequently occluded by the hand or itself during interaction, making it difficult to estimate its pose accurately when only a portion of the object is visible. Videos, however, offer the advantage of capturing different views and movements of the object, which can reveal parts that might be occluded or blurred in a single frame. This advantage motivates the use of a transformer encoder to model feature interactions along the temporal dimension. Unlike traditional recurrent architectures that process frames sequentially, the transformer's self-attention mechanism allows each frame to leverage information from all other frames directly, regardless of their position in the sequence \cite{vaswani2017attention}. This approach enables the model to capture long-range temporal dependencies more effectively. Similar to the spatial transformer encoder, the temporal transformer encoder processes the sequence of features $\lbrace\mathcal{F}^{se}_t\rbrace_{t=1}^{T}$, which are enhanced by the spatial transformer, and outputs a new set of latent vectors $\lbrace\mathcal{F}^{t}_t\rbrace_{t=1}^{T}$. Given $\mathcal{F}^t_t$, an MLP (Multi-Layer Perceptron) serves as the pose regression network, processing these features to predict the initial object pose parameters $\hat{\mathbf{P}}_t$. By incorporating information across multiple frames, the model can more accurately predict the object's pose, even in cases where individual frames are challenging to interpret due to rapid motion. 

\subsection{Visibility-Aware Object Pose Estimation Under Occlusions}

Our proposed framework leverages spatial and temporal transformers to improve object pose estimation accuracy. However, predicting object poses under heavy occlusion remains particularly challenging due to the lack of sufficient image evidence within a single frame. To address this issue, we propose a novel approach that combines temporally-aware features, $\mathcal{F}^t_t$, from the current occluded frame with object pose information from other visible frames to predict the pose of the occluded object. This is achieved by first estimating visibility scores for each frame, which are subsequently used to guide the aggregation of pose evidence from neighboring visible frames.

\subsubsection{Visibility Estimation}

We introduce a visibility score $v_t \in [0, 1]$ for each frame $t$, which quantifies the visibility of the object in that frame. This score is crucial for identifying heavily occluded frames where direct pose estimation is unreliable. The visibility score is predicted using a visibility decoder $f_{vis}(\cdot)$, which operates on temporally-aware features $\mathcal{F}^t_t$. The decoder consists of fully connected layers, culminating in a sigmoid activation function to ensure outputs lie within the range $[0, 1]$:

\begin{equation}
v_t = \sigma(f_{vis}(\mathcal{F}^t_t)),
\end{equation}

\noindent where $\sigma(\cdot)$ denotes the sigmoid function. The use of the sigmoid activation function is motivated by its ability to map outputs to a probability-like range of $[0, 1]$, making it particularly suitable for modeling visibility as a confidence score. This ensures interpretability of the output, where a value close to $1$ indicates high visibility, and a value close to $0$ indicates low visibility. Additionally, the sigmoid function provides a smooth gradient, which aids in stable optimization during training using the Binary Cross Entropy (BCE) loss.

The visibility score $v_t$ represents the confidence level of object visibility in frame $t$. For training, the ground truth visibility score $v_t$ is computed as the fraction of visible object pixels in frame $t$, defined as:

\begin{equation}
v_t = \frac{|O_t|}{|O|},
\end{equation}

\noindent where $O$ is the total set of annotated object pixels, and $O_t$ represents the subset of pixels visible in frame $t$. The visibility estimation network is trained using the BCE loss, which measures the difference between predicted visibility scores $\hat{v}_t$ and ground truth scores $v_t$:

\begin{equation}
\mathcal{L}_{vis} = - \frac{1}{N} \sum_{t=1}^{N} \left( v_t \log(\hat{v}_t) + (1 - v_t) \log(1 - \hat{v}_t) \right),
\end{equation}

\noindent where $N$ is the total number of frames in the dataset. The BCE loss was chosen because it is well-suited for tasks where the target values represent probabilities or binary labels. It penalizes incorrect predictions while ensuring the model outputs confidence values that align with the ground truth visibility. Together, the sigmoid activation and BCE loss enable the network to effectively learn and predict visibility scores for frames with varying levels of occlusion.

\subsubsection{Object Pose Prediction Under Heavy Occlusion}

Accurate object pose prediction for heavily occluded frames (visibility score $v_i < \delta$, where $\delta = 0.5$) is achieved by aggregating temporal pose information from visible frames. The Pose Transformer is used to integrate pose embeddings from multiple visible frames, ensuring consistent pose estimation even under occlusions. Temporally-aware features from occluded frames, $\mathcal{F}^t_t$, are denoted as $\mathcal{F}_{occ} \in \mathbb{R}^C$.

To aggregate pose information, visible-frame poses $\hat{\mathbf{P}}_t$ are embedded into a high-dimensional space, resulting in pose embeddings $\mathbf{E}_t \in \mathbb{R}^d$. These embeddings are processed by the Pose Transformer, which uses a multi-head self-attention mechanism to compute attention scores $\alpha_{ij}$ between embeddings:

\begin{equation}
\alpha_{ij} = \text{softmax}\left(\frac{\mathbf{Q}_i \cdot \mathbf{K}_j^T}{\sqrt{d_k}}\right),
\end{equation}

\noindent where $\mathbf{Q}_i = \mathbf{E}_i \mathbf{W}_Q$, $\mathbf{K}_j = \mathbf{E}_j \mathbf{W}_K$, and $\mathbf{V}_j = \mathbf{E}_j \mathbf{W}_V$ are query, key, and value vectors, respectively. These are obtained using learnable weight matrices $\mathbf{W}_Q, \mathbf{W}_K, \mathbf{W}_V \in \mathbb{R}^{d \times d_k}$, and $d_k = d / \text{num\_heads}$.

The transformer outputs an aggregated pose feature $\mathbf{Z}_{agg} \in \mathbb{R}^d$, which integrates temporal information from the visible frames. This feature is combined with the occluded frame's feature vector $\mathcal{F}_{occ}$ through a cross-attention mechanism, where $\mathbf{Z}_{agg}$ serves as the query and $\mathcal{F}_{occ}$ provides the keys and values. Specifically, the query is:

\begin{equation}
\mathbf{Q}_{agg} = \mathbf{Z}_{agg} \mathbf{W}_Q' \in \mathbb{R}^{d_q},
\end{equation}

\noindent while keys and values are obtained as:

\begin{equation}
\mathbf{K}_{occ} = \mathcal{F}_{occ} \mathbf{W}_K' \in \mathbb{R}^{d_k},
\end{equation}

\begin{equation}
\mathbf{V}_{occ} = \mathcal{F}_{occ} \mathbf{W}_V' \in \mathbb{R}^{d_v}.
\end{equation}

\noindent The cross-attention weights are computed as:

\begin{equation}
\beta_{ij} = \text{softmax}\left(\frac{\mathbf{Q}_{agg} \cdot \mathbf{K}_{occ}^T}{\sqrt{d_k}}\right),
\end{equation}

\noindent resulting in the fused feature representation:

\begin{equation}
\mathcal{F}_{fused} = \sum_{j=1}^{C} \beta_{ij} \mathbf{V}_{occ}.
\end{equation}

\noindent The fused feature $\mathcal{F}_{fused}$ integrates information from both visible and occluded frames, enabling accurate pose predictions under occlusion. Finally, a multi-layer perceptron (MLP) processes $\mathcal{F}_{fused}$ to predict the object's pose $\hat{\mathbf{P}}_{occ}$ for the occluded frame. The use of an MLP for this task is motivated by its ability to model complex, non-linear relationships in high-dimensional feature spaces. Specifically, the MLP acts as a flexible and computationally efficient mapping function, translating the fused feature $\mathcal{F}_{fused}$ into the 6D pose parameters (rotation and translation). Its architecture typically consists of multiple fully connected layers, interleaved with activation function ReLU, which enable the network to capture hierarchical representations of the fused information. This hierarchical modeling is particularly advantageous for tasks like pose estimation, where subtle variations in the input features can correspond to significant changes in the output pose.

\subsection{Loss Function}

For asymmetric objects, the loss function is defined as the average Euclidean distance between the model points transformed by the ground truth pose and the predicted pose:

\begin{equation}
\mathcal{L}_{pose} = \frac{1}{N} \sum_{i=1}^{N} \left\| (\mathbf{R} \mathbf{x}_i + \mathbf{t}) - (\hat{\mathbf{R}} \mathbf{x}_i + \hat{\mathbf{t}}) \right\|_2,
\end{equation}

\noindent where:

\begin{itemize}
    \item $\mathbf{R}$ and $\mathbf{t}$ are the ground truth rotation matrix and translation vector.
    \item $\hat{\mathbf{R}}$ and $\hat{\mathbf{t}}$ are the predicted rotation matrix and translation vector.
    \item $\mathbf{x}_i$ represents the $i$-th point in the object's model.
    \item $N$ is the total number of points sampled on the object model.
    \item $\left\| \cdot \right\|_2$ is the Euclidean norm, measuring the distance between the two sets of transformed points.
\end{itemize}

\noindent For symmetric objects, the above loss function is not well-defined due to the potential for multiple canonical frames. Instead, we use a modified loss function that minimizes the distance between each point on the estimated model orientation and the closest point on the ground truth model:

\begin{equation}
\mathcal{L}_{pose} = \frac{1}{N} \sum_{i=1}^{N} \min_{j} \left\| (\mathbf{R} \mathbf{x}_j + \mathbf{t}) - (\hat{\mathbf{R}} \mathbf{x}_i + \hat{\mathbf{t}}) \right\|_2.
\end{equation}

\noindent In addition to the pose loss, we introduce a keypoint-based loss function. Keypoints are defined as distinctive points on the object that are consistent across frames. The keypoint loss is computed as the average Euclidean distance between the predicted keypoints $\hat{\mathbf{k}}_i$ and the ground truth keypoints $\mathbf{k}_i$:

\begin{equation}
\mathcal{L}_{keypoint} = \frac{1}{M} \sum_{i=1}^{M} \left\| \mathbf{k}_i - \hat{\mathbf{k}}_i \right\|_2,
\end{equation}

\noindent where:

\begin{itemize}
    \item $\mathbf{k}_i$ represents the $i$-th ground truth keypoint.
    \item $\hat{\mathbf{k}}_i$ is the corresponding predicted keypoint.
    \item $M$ is the total number of keypoints.
\end{itemize}

\noindent The total loss function now combines the pose estimation loss, the keypoint loss, and the visibility-aware loss:

\begin{equation}
\mathcal{L}_{total} = \mathcal{L}_{pose} + \lambda_{key} \mathcal{L}_{keypoint} + \lambda_{vis} \mathcal{L}_{vis},
\end{equation}

\noindent where $\lambda_{key}$ and $\lambda_{vis}$ are weights that balance the contribution of the keypoint loss and the visibility-aware loss in the total loss function.
