\section{Related Work}
\label{sec:Related}

\subsection{Object Pose Estimation from a Single Image}

Vision-based object pose estimation involves detecting an object of interest and estimating its 6-DoF (Degrees of Freedom) pose, which includes 3D rotation $\mathbf{R} \in SO(3)$ and 3D translation $\mathbf{t} \in \mathbb{R}^3$, relative to a canonical frame. The assumption is that the CAD model of the object is available beforehand. Depending on the input data format, object pose estimation methods can be classified into three categories: depth-based methods \cite{wang20216d, gao20206d, guo2021efficient}, RGB-based methods \cite{billings2019silhonet, peng2019pvnet, wang2021gdr}, and RGBD-based methods \cite{wang2019densefusion, he2020pvn3d, hong2024rdpn6d}. Depth-based and RGBD-based methods rely on depth images captured by depth cameras, which limits their applicability in certain scenarios, as depth cameras are not widely used in many applications. In contrast, RGB-based methods require only a single RGB image, making them more practical and accessible for real-world use. However, the challenge of estimating the 3D pose from a single RGB image stems from the loss of depth information and ambiguities in perspective projection.

Traditional methods \cite{lowe1999object, lepetit2005monocular} approached this problem by establishing correspondences between 2D image features and 3D points on the object model. These approaches rely on handcrafted features, but they are susceptible to image variations, background clutter, and occlusions. The advent of deep learning has shifted the focus towards learning-based methods, which have demonstrated superior performance by leveraging large datasets and powerful convolutional neural networks (CNNs). Deep learning-based methods for object pose estimation can be broadly categorized into three classes: direct methods \cite{xiang2017posecnn, kehl2017ssd, trabelsi2021pose, wang2021gdr}, keypoint-based methods \cite{rad2017bb8, tekin2018real, oberweger2018making, peng2019pvnet}, and dense coordinate-based methods \cite{li2019cdpn, park2019pix2pose, zakharov2019dpod}. Direct methods treat pose estimation as a regression or classification task, directly predicting pose parameters from input images \cite{xiang2017posecnn, kehl2017ssd, trabelsi2021pose}. While these methods are intuitive, they often require additional time-consuming pose refinement operations to improve accuracy. To address these limitations, other approaches \cite{hu2020single, chen2020end, wang2021gdr} have modified indirect methods into direct methods by leveraging neural networks to establish 2D-3D correspondences, followed by solving the Perspective-n-Point (PnP) problem \cite{lepetit2009ep} within a deep learning framework. These methods, although more efficient, often underperform compared to other approaches due to their reliance on deep networks alone to recover the full 6-DoF parameters. In contrast to directly predicting pose-related parameters, correspondence-based methods, such as keypoint-based methods \cite{rad2017bb8, tekin2018real, oberweger2018making, peng2019pvnet}, have shown superior performance by building 2D-3D correspondences. Keypoint-based methods utilize CNNs to detect 2D keypoints in the image, followed by solving the PnP problem to estimate the object's pose \cite{lepetit2009ep}. These methods achieve notable success, but they require predefined keypoints for each object, leading to high labor costs. Beyond keypoint-based methods, dense coordinate-based methods \cite{li2019cdpn, park2019pix2pose, zakharov2019dpod} represent another class of correspondence-based approaches. These methods frame the 6-DoF pose estimation task as building dense 2D-3D correspondences, followed by a PnP solver to estimate the object's pose. Dense correspondences are obtained by predicting 3D object coordinates for each object pixel or by generating dense UV maps. However, predicting object coordinates for dense correspondences is more challenging than detecting sparse keypoints due to the larger search space involved.

Despite the recent progress, object pose estimation from a single image remains a difficult and unsolved problem, particularly in scenarios involving heavy occlusion or rapid motion, such as hand-held objects. To address these challenges, our proposed method leverages a sequence of images rather than a single image to estimate the in-hand object pose, which improves robustness and accuracy in dynamic environments.

\subsection{Deep Learning-based Object Pose Tracking}

Object pose tracking extends the problem of pose estimation to a temporal sequence of images, where the goal is to maintain an accurate estimate of an object's 3D pose across a video sequence \cite{sun2021robust, huang2021pixel, stoiber2020sparse, tian2022large}. Deep learning-based methods for object pose tracking are generally classified into two categories: tracking by refinement \cite{garon2017deep, marougkas2020track, manhardt2018deep, li2018deepim, dosovitskiy2015flownet, wen2020se} and tracking by optimization \cite{deng2021poserbpf, majcher20203d, zhong2020seeing, wang2023deep}.

Tracking by refinement methods initialize the object's pose in each frame using the pose estimate from the previous frame and then refine it based on the new input. Garon et al. \cite{garon2017deep} pioneered this approach by using the pose from the last frame to render a synthetic RGB-D frame of the target object. Their network then takes the rendered image and the observed image as inputs to predict the relative 6-DoF pose between these frames. To improve performance in cluttered and occluded environments, Marougkas et al. \cite{marougkas2020track} introduced multiple parallel soft spatial attention modules into the network architecture. While the above methods rely on RGB-D data, Manhardt et al. \cite{manhardt2018deep} proposed a refinement approach that works solely with RGB images. They introduced a new visual loss function that drives the pose update process by aligning object contours in the image, avoiding the need for explicit appearance models or depth data. These refinement methods typically utilize the pose from the last frame as an initialization for the current frame, refining it through a single forward pass in a neural network. However, this process can lead to coarse pose predictions due to the limited refinement performed. To address this, DeepIM \cite{li2018deepim} was introduced as a method for iterative refinement. In DeepIM, the network iteratively matches the rendered image against the observed image, predicting the relative pose between the two using a FlowNet-like architecture \cite{dosovitskiy2015flownet}. Although DeepIM demonstrates strong performance, it requires extensive training on real-world images and suffers from slow runtime during inference. Wen et al. \cite{wen2020se}, on the other hand, achieves real-time performance by training solely on synthetic data. Its success is attributed to techniques such as rotation-translation disentanglement, the Lie algebra representation of rotation, and DR data enhancement. While tracking by refinement methods generally achieve high accuracy, they depend on rendering CAD models into synthetic images, which is often non-differentiable, hindering end-to-end training. Additionally, the quality of the rendering and the CAD models themselves can significantly impact tracking performance. 

In contrast to refinement-based methods, tracking by optimization focuses on formulating the pose tracking problem as an optimization task. PoseRBPF \cite{deng2021poserbpf} exemplifies this approach by combining Rao-Blackwellized particle filtering with a learned autoencoder network for pose updates. In this framework, the particle filter estimates discretized distributions over rotations and translations, with translations updated directly by the filter. Rotations are updated by comparing image descriptors with precomputed entries in a codebook generated by the autoencoder. While PoseRBPF performs well, it requires discretization of the continuous rotation space to build the codebook, which limits its performance and increases the algorithm's time complexity as the number of discretized rotations grows. Majcher et al. \cite{majcher20203d} also utilize particle filtering in their tracking framework. In their approach, a U-Net is used to segment the target object in advance, and the object's 6-DoF pose in the current frame is estimated by matching the segmented object with a rendered image based on the pose from the last frame. The final pose is obtained by optimizing the difference between object silhouettes and edges, with the particle filter estimating the posterior probability distribution. Zhong et al. \cite{zhong2020seeing} introduced a method that integrates a learning-based video segmentation module into an optimization-based pose estimation pipeline. Wang et al. \cite{wang2023deep} introduced a three-phase pipeline for object pose tracking. The process begins with an FPN-Lite network integrated with MobileNetV2 to extract multi-level features from the current image and to project a 3D object model for deriving 2D contours based on the pose from the previous frame. Next, a boundary prediction network is employed, which takes features from local regions around the contours as input and outputs a probability distribution representing the true boundary locations. Finally, the 6-DoF object pose is refined using Newton's method, leveraging the boundary probability distribution. The primary advantage of tracking by optimization methods lies in their interpretability. However, the optimization process is typically slow, and challenges such as occlusion remain problematic for many methods in this category.

\subsection{Transformer-based Methods}

The Transformer model \cite{vaswani2017attention} has emerged as the dominant architecture for a wide range of natural language processing (NLP) tasks. Unlike CNNs, which rely on strong inductive biases, the Transformer's self-attention mechanism is highly adaptable, allowing it to focus on different parts of a sequence of features. Transformer-based methods have recently been introduced to the field of computer vision, offering a powerful alternative to traditional CNNs. The Vision Transformer (ViT) \cite{dosovitskiy2020image} is a direct adaptation of the transformer architecture to image processing. Instead of using convolutional layers, ViTs divide an image into fixed-size patches, linearly embed each patch into a vector, and then apply the transformer architecture to these vectors. Self-attention allows the model to capture global dependencies and relationships between different parts of the image, which can be beneficial for tasks like image classification and object detection. Some approaches combine traditional convolutional neural networks (CNNs) with transformers \cite{liu2021swin, guo2022cmt}. Hybrid models use CNNs to extract initial features from images and then apply self-attention mechanisms to these features. This hybrid approach leverages the strengths of both architectures: the locality and efficiency of CNNs and the global context understanding of transformers. In this work, we also emplopy a hybrid model. While transformer-based methods are still relatively new in the domain of object pose estimation and tracking, they represent a promising direction for future research. Their ability to model global context and capture intricate relationships within the data makes them well-suited for advancing the state of the art in both single-image pose estimation and pose tracking.
