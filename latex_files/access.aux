\relax 
\citation{marullo20236d,du2021vision,cho2024integration}
\citation{hoang2024graspability,castro2023crt,peng2019pvnet}
\citation{marullo20236d}
\citation{chao2021dexycb,garcia2018first,llop2022benchmarking}
\citation{billings2019silhonet,peng2019pvnet}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{}\protected@file@percent }
\newlabel{sec:intro}{{I}{1}}
\citation{doosti2020hope,lin2023harmonious,wang2023interacting,woo2023survey}
\citation{romero2022embodied}
\citation{son2022past}
\citation{park2015spatial}
\citation{wang20216d,gao20206d,guo2021efficient}
\citation{billings2019silhonet,peng2019pvnet,wang2021gdr}
\citation{wang2019densefusion,he2020pvn3d,hong2024rdpn6d}
\citation{lowe1999object,lepetit2005monocular}
\citation{xiang2017posecnn,kehl2017ssd,trabelsi2021pose,wang2021gdr}
\citation{rad2017bb8,tekin2018real,oberweger2018making,peng2019pvnet}
\citation{li2019cdpn,park2019pix2pose,zakharov2019dpod}
\citation{xiang2017posecnn,kehl2017ssd,trabelsi2021pose}
\citation{hu2020single,chen2020end,wang2021gdr}
\citation{lepetit2009ep}
\citation{rad2017bb8,tekin2018real,oberweger2018making,peng2019pvnet}
\citation{lepetit2009ep}
\citation{li2019cdpn,park2019pix2pose,zakharov2019dpod}
\citation{sun2021robust,huang2021pixel,stoiber2020sparse,tian2022large}
\citation{garon2017deep,marougkas2020track,manhardt2018deep,li2018deepim,dosovitskiy2015flownet,wen2020se}
\citation{deng2021poserbpf,majcher20203d,zhong2020seeing,wang2023deep}
\citation{garon2017deep}
\citation{marougkas2020track}
\citation{manhardt2018deep}
\citation{li2018deepim}
\citation{dosovitskiy2015flownet}
\citation{wen2020se}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work}{3}{}\protected@file@percent }
\newlabel{sec:Related}{{II}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-A}}Object Pose Estimation from a Single Image}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-B}}Deep Learning-based Object Pose Tracking}{3}{}\protected@file@percent }
\citation{deng2021poserbpf}
\citation{majcher20203d}
\citation{zhong2020seeing}
\citation{wang2023deep}
\citation{vaswani2017attention}
\citation{dosovitskiy2020image}
\citation{liu2021swin,guo2022cmt}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-C}}Transformer-based Methods}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {III}Method}{4}{}\protected@file@percent }
\citation{carion2020end}
\citation{lin2017feature}
\citation{he2016deep}
\citation{vaswani2017attention}
\citation{oberweger2018making,tekin2018real,rad2017bb8}
\citation{jaegle2021perceiver}
\citation{vaswani2017attention}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Overview of the proposed framework for in-hand object pose tracking. Our approach leverages transformers to capture both spatial relationships within individual frames and temporal dependencies across image sequences, enabling robust frame-wise object pose estimation. To address challenges posed by heavy occlusion or motion blur, a visibility-aware module integrates temporally-aware features and aggregates pose information from neighboring frames. This allows the model to maintain accurate pose estimates even in difficult scenarios where direct visual cues are limited or absent.\relax }}{5}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:overview}{{1}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-A}}Spatial-Temporal Transformer}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-A}1}CNN Backbone}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-A}2}Spatial Transformer}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-A}3}Temporal Transformer}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-B}}Visibility-aware Object Pose Estimation under occlusions}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-B}1}Visibility Estimation.}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-B}2}Object Pose Prediction Under Heavy Occlusion.}{6}{}\protected@file@percent }
\citation{chao2021dexycb}
\citation{garcia2018first}
\citation{hampali2020honnotate}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-C}}Loss Function}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {IV}Evaluation}{7}{}\protected@file@percent }
\newlabel{sec:Evaluation}{{IV}{7}}
\citation{chao2021dexycb}
\citation{garcia2018first}
\citation{hampali2020honnotate}
\citation{he2016deep}
\citation{he2017mask}
\citation{wang2023deep}
\citation{wang2023deep}
\citation{castro2023crt}
\citation{castro2023crt}
\citation{castro2023crt}
\citation{wang2023deep}
\citation{wang2023deep}
\citation{castro2023crt}
\citation{castro2023crt}
\citation{wang2023deep}
\citation{wang2023deep}
\citation{castro2023crt}
\citation{wang2023deep}
\citation{wang2023deep}
\citation{castro2023crt}
\citation{castro2023crt}
\citation{castro2023crt}
\citation{wang2023deep}
\citation{castro2023crt}
\citation{wang2023deep}
\citation{hinterstoisser2012model}
\citation{bregier2017symmetry}
\citation{bregier2017symmetry}
\citation{wang2019densefusion}
\citation{chao2021dexycb}
\citation{garcia2018first}
\citation{hampali2020honnotate}
\citation{billings2019silhonet,peng2019pvnet,wang2021gdr,castro2023crt}
\citation{sun2021robust,huang2021pixel,he2021ffb6d,stoiber2020sparse,stoiber2022srt3d,tian2022large,wang2023deep}
\citation{chao2021dexycb}
\citation{garcia2018first}
\citation{hampali2020honnotate}
\citation{billings2019silhonet,peng2019pvnet,wang2021gdr,castro2023crt}
\citation{sun2021robust,huang2021pixel,he2021ffb6d,stoiber2020sparse,stoiber2022srt3d,tian2022large,wang2023deep}
\citation{billings2019silhonet}
\citation{peng2019pvnet}
\citation{wang2021gdr}
\citation{castro2023crt}
\citation{sun2021robust}
\citation{huang2021pixel}
\citation{stoiber2020sparse}
\citation{stoiber2022srt3d}
\citation{tian2022large}
\citation{wang2023deep}
\citation{chao2021dexycb}
\citation{garcia2018first}
\citation{hampali2020honnotate}
\citation{billings2019silhonet}
\citation{peng2019pvnet}
\citation{wang2021gdr}
\citation{castro2023crt}
\citation{wang2023deep}
\citation{castro2023crt}
\citation{wang2021gdr}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-A}}Datasets}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-B}}Implementation Details}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-C}}Evaluation Metric}{8}{}\protected@file@percent }
\newlabel{sec:metric}{{\mbox  {IV-C}}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-D}}Result}{8}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Qualitative Comparison of the state-of-the-art single-view method \cite  {castro2023crt}, video-based method \cite  {wang2023deep}, and our method. The proposed method robustly generates more stable and accurate poses over time than previous approaches. (a) are the input RGBD images. (b) shows the rendered images using ground truth hand and object poses. (c), (d), and (e) display the rendered images using ground truth hand poses and object poses predicted by our method, \cite  {wang2023deep}, and \cite  {castro2023crt}, respectively.\relax }}{9}{}\protected@file@percent }
\newlabel{fig:result1}{{2}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Qualitative Comparison of the state-of-the-art single-view method \cite  {castro2023crt}, video-based method \cite  {wang2023deep}, and our method. Here we only show results on frames under heavy occlusion (frames with a visibility score \( v_i \) smaller than \( \delta = 0.5 \)).\relax }}{10}{}\protected@file@percent }
\newlabel{fig:result2}{{3}{10}}
\citation{chao2021dexycb}
\citation{garcia2018first}
\citation{hampali2020honnotate}
\citation{billings2019silhonet,peng2019pvnet,wang2021gdr,castro2023crt}
\citation{sun2021robust,huang2021pixel,he2021ffb6d,stoiber2020sparse,stoiber2022srt3d,tian2022large,wang2023deep}
\citation{chao2021dexycb}
\citation{garcia2018first}
\citation{hampali2020honnotate}
\citation{billings2019silhonet,peng2019pvnet,wang2021gdr,castro2023crt}
\citation{sun2021robust,huang2021pixel,he2021ffb6d,stoiber2020sparse,stoiber2022srt3d,tian2022large,wang2023deep}
\citation{billings2019silhonet}
\citation{peng2019pvnet}
\citation{wang2021gdr}
\citation{castro2023crt}
\citation{sun2021robust}
\citation{huang2021pixel}
\citation{stoiber2020sparse}
\citation{stoiber2022srt3d}
\citation{tian2022large}
\citation{wang2023deep}
\citation{wang2023deep}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Quantitative results on the DexYCB \cite  {chao2021dexycb}, FPHAB \cite  {garcia2018first}, and HO-3D \cite  {hampali2020honnotate} datasets. Single image -based methods \cite  {billings2019silhonet, peng2019pvnet, wang2021gdr, castro2023crt}, and tracking methods \cite  {sun2021robust, huang2021pixel, he2021ffb6d, stoiber2020sparse, stoiber2022srt3d, tian2022large, wang2023deep} are compared with our proposed method (Ours).\relax }}{11}{}\protected@file@percent }
\newlabel{tab:dataset_ex}{{1}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-E}}Ablation Study}{11}{}\protected@file@percent }
\bibstyle{IEEEtran}
\bibdata{References}
\bibcite{marullo20236d}{1}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Quantitative results on the DexYCB \cite  {chao2021dexycb}, FPHAB \cite  {garcia2018first}, and HO-3D \cite  {hampali2020honnotate} datasets. However, here we only evaluate methods on frames under heavy occlusion (frames with a visibility score \( v_i \) smaller than \( \delta = 0.5 \)). Single image-based methods \cite  {billings2019silhonet, peng2019pvnet, wang2021gdr, castro2023crt}, and tracking methods \cite  {sun2021robust, huang2021pixel, he2021ffb6d, stoiber2020sparse, stoiber2022srt3d, tian2022large, wang2023deep} are compared with our proposed method (Ours).\relax }}{12}{}\protected@file@percent }
\newlabel{tab:occlusion_ex}{{2}{12}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Ablation Study. $\mathcal  {M}_{s}$ spatial transformer, $\mathcal  {M}_{t}$ temporal transformer, $\mathcal  {M}_{va}$ visibility-aware object pose estimation module.\relax }}{12}{}\protected@file@percent }
\newlabel{tab:ablation_ex}{{3}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Conclusion}{12}{}\protected@file@percent }
\bibcite{du2021vision}{2}
\bibcite{cho2024integration}{3}
\bibcite{hoang2024graspability}{4}
\bibcite{castro2023crt}{5}
\bibcite{peng2019pvnet}{6}
\bibcite{chao2021dexycb}{7}
\bibcite{garcia2018first}{8}
\bibcite{llop2022benchmarking}{9}
\bibcite{billings2019silhonet}{10}
\bibcite{doosti2020hope}{11}
\bibcite{lin2023harmonious}{12}
\bibcite{wang2023interacting}{13}
\bibcite{woo2023survey}{14}
\bibcite{romero2022embodied}{15}
\bibcite{son2022past}{16}
\bibcite{park2015spatial}{17}
\bibcite{wang20216d}{18}
\bibcite{gao20206d}{19}
\bibcite{guo2021efficient}{20}
\bibcite{wang2021gdr}{21}
\bibcite{wang2019densefusion}{22}
\bibcite{he2020pvn3d}{23}
\bibcite{hong2024rdpn6d}{24}
\bibcite{lowe1999object}{25}
\bibcite{lepetit2005monocular}{26}
\bibcite{xiang2017posecnn}{27}
\bibcite{kehl2017ssd}{28}
\bibcite{trabelsi2021pose}{29}
\bibcite{rad2017bb8}{30}
\bibcite{tekin2018real}{31}
\bibcite{oberweger2018making}{32}
\bibcite{li2019cdpn}{33}
\bibcite{park2019pix2pose}{34}
\bibcite{zakharov2019dpod}{35}
\bibcite{hu2020single}{36}
\bibcite{chen2020end}{37}
\bibcite{lepetit2009ep}{38}
\bibcite{sun2021robust}{39}
\bibcite{huang2021pixel}{40}
\bibcite{stoiber2020sparse}{41}
\bibcite{tian2022large}{42}
\bibcite{garon2017deep}{43}
\bibcite{marougkas2020track}{44}
\@writefile{toc}{\contentsline {section}{REFERENCES}{13}{}\protected@file@percent }
\bibcite{manhardt2018deep}{45}
\bibcite{li2018deepim}{46}
\bibcite{dosovitskiy2015flownet}{47}
\bibcite{wen2020se}{48}
\bibcite{deng2021poserbpf}{49}
\bibcite{majcher20203d}{50}
\bibcite{zhong2020seeing}{51}
\bibcite{wang2023deep}{52}
\bibcite{vaswani2017attention}{53}
\bibcite{dosovitskiy2020image}{54}
\bibcite{liu2021swin}{55}
\bibcite{guo2022cmt}{56}
\bibcite{carion2020end}{57}
\bibcite{lin2017feature}{58}
\bibcite{he2016deep}{59}
\bibcite{jaegle2021perceiver}{60}
\bibcite{hampali2020honnotate}{61}
\bibcite{he2017mask}{62}
\bibcite{hinterstoisser2012model}{63}
\bibcite{bregier2017symmetry}{64}
\bibcite{he2021ffb6d}{65}
\bibcite{stoiber2022srt3d}{66}
\@writefile{toc}{\contentsline {subsection}{Phan Xuan Tan}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Dinh-Cuong Hoang}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Eiji Kamioka}{14}{}\protected@file@percent }
\gdef \@abspage@last{14}
