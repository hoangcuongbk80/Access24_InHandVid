\section{Related Work}
\label{sec:Related}

\subsection{Object Pose Estimation from a Single Image}

Vision-based object pose estimation involves detecting an object of interest and estimating its 6-DoF (Degrees of Freedom) pose, which includes 3D rotation $\mathbf{R} \in SO(3)$ and 3D translation $\mathbf{t} \in \mathbb{R}^3$, relative to a canonical frame. The assumption is that the CAD model of the object is available beforehand. Depending on the input data format, object pose estimation methods can be classified into three categories: depth-based methods \cite{wang20216d, gao20206d, guo2021efficient}, RGB-based methods \cite{billings2019silhonet, peng2019pvnet, wang2021gdr}, and RGBD-based methods \cite{wang2019densefusion, he2020pvn3d, hong2024rdpn6d}. Depth-based and RGBD-based methods rely on depth images captured by depth cameras, which limits their applicability in certain scenarios, as depth cameras are not widely used in many applications. In contrast, RGB-based methods require only a single RGB image, making them more practical and accessible for real-world use. However, the challenge of estimating the 3D pose from a single RGB image stems from the loss of depth information and ambiguities in perspective projection.

Traditional methods \cite{lowe1999object, lepetit2005monocular} approached this problem by establishing correspondences between 2D image features and 3D points on the object model. These approaches rely on handcrafted features, but they are susceptible to image variations, background clutter, and occlusions. The advent of deep learning has shifted the focus towards learning-based methods, which have demonstrated superior performance by leveraging large datasets and powerful convolutional neural networks (CNNs). Deep learning-based methods for object pose estimation can be broadly categorized into three classes: direct methods \cite{xiang2017posecnn, kehl2017ssd, trabelsi2021pose, wang2021gdr}, keypoint-based methods \cite{rad2017bb8, tekin2018real, oberweger2018making, peng2019pvnet}, and dense coordinate-based methods \cite{li2019cdpn, park2019pix2pose, zakharov2019dpod}. Direct methods treat pose estimation as a regression or classification task, directly predicting pose parameters from input images \cite{xiang2017posecnn, kehl2017ssd, trabelsi2021pose}. While these methods are intuitive, they often require additional time-consuming pose refinement operations to improve accuracy. To address these limitations, other approaches \cite{hu2020single, chen2020end, wang2021gdr} have modified indirect methods into direct methods by leveraging neural networks to establish 2D-3D correspondences, followed by solving the Perspective-n-Point (PnP) problem \cite{lepetit2009ep} within a deep learning framework. These methods, although more efficient, often underperform compared to other approaches due to their reliance on deep networks alone to recover the full 6-DoF parameters. In contrast to directly predicting pose-related parameters, correspondence-based methods, such as keypoint-based methods \cite{rad2017bb8, tekin2018real, oberweger2018making, peng2019pvnet}, have shown superior performance by building 2D-3D correspondences. Keypoint-based methods utilize CNNs to detect 2D keypoints in the image, followed by solving the PnP problem to estimate the object's pose \cite{lepetit2009ep}. These methods achieve notable success, but they require predefined keypoints for each object, leading to high labor costs. Beyond keypoint-based methods, dense coordinate-based methods \cite{li2019cdpn, park2019pix2pose, zakharov2019dpod} represent another class of correspondence-based approaches. These methods frame the 6-DoF pose estimation task as building dense 2D-3D correspondences, followed by a PnP solver to estimate the object's pose. Dense correspondences are obtained by predicting 3D object coordinates for each object pixel or by generating dense UV maps. However, predicting object coordinates for dense correspondences is more challenging than detecting sparse keypoints due to the larger search space involved.

Despite the recent progress, object pose estimation from a single image remains a difficult and unsolved problem, particularly in scenarios involving heavy occlusion or rapid motion, such as hand-held objects. To address these challenges, our proposed method leverages a sequence of images rather than a single image to estimate the in-hand object pose, which improves robustness and accuracy in dynamic environments.

\subsection{Deep Learning-based Object Pose Tracking}

Object pose tracking extends the problem of pose estimation to a temporal sequence of images, where the goal is to maintain an accurate estimate of an object's 3D pose across a video sequence \cite{sun2021robust, huang2021pixel, stoiber2020sparse, tian2022large}. Deep learning-based methods for object pose tracking are generally classified into two categories: tracking by refinement \cite{garon2017deep, marougkas2020track, manhardt2018deep, li2018deepim, dosovitskiy2015flownet, wen2020se} and tracking by optimization \cite{deng2021poserbpf, majcher20203d, zhong2020seeing, wang2023deep}.

Tracking by refinement methods initialize the object's pose in each frame using the pose estimate from the previous frame and then refine it based on the new input. Garon et al. \cite{garon2017deep} pioneered this approach by using the pose from the last frame to render a synthetic RGB-D frame of the target object. Their network then takes the rendered image and the observed image as inputs to predict the relative 6-DoF pose between these frames. To improve performance in cluttered and occluded environments, Marougkas et al. \cite{marougkas2020track} introduced multiple parallel soft spatial attention modules into the network architecture. While the above methods rely on RGB-D data, Manhardt et al. \cite{manhardt2018deep} proposed a refinement approach that works solely with RGB images. They introduced a new visual loss function that drives the pose update process by aligning object contours in the image, avoiding the need for explicit appearance models or depth data. These refinement methods typically utilize the pose from the last frame as an initialization for the current frame, refining it through a single forward pass in a neural network. However, this process can lead to coarse pose predictions due to the limited refinement performed. To address this, DeepIM \cite{li2018deepim} was introduced as a method for iterative refinement. In DeepIM, the network iteratively matches the rendered image against the observed image, predicting the relative pose between the two using a FlowNet-like architecture \cite{dosovitskiy2015flownet}. Although DeepIM demonstrates strong performance, it requires extensive training on real-world images and suffers from slow runtime during inference. Wen et al. \cite{wen2020se}, on the other hand, achieves real-time performance by training solely on synthetic data. Its success is attributed to techniques such as rotation-translation disentanglement, the Lie algebra representation of rotation, and DR data enhancement. While tracking by refinement methods generally achieve high accuracy, they depend on rendering CAD models into synthetic images, which is often non-differentiable, hindering end-to-end training. Additionally, the quality of the rendering and the CAD models themselves can significantly impact tracking performance. 

In contrast to refinement-based methods, tracking by optimization focuses on formulating the pose tracking problem as an optimization task. PoseRBPF \cite{deng2021poserbpf} exemplifies this approach by combining Rao-Blackwellized particle filtering with a learned autoencoder network for pose updates. In this framework, the particle filter estimates discretized distributions over rotations and translations, with translations updated directly by the filter. Rotations are updated by comparing image descriptors with precomputed entries in a codebook generated by the autoencoder. While PoseRBPF performs well, it requires discretization of the continuous rotation space to build the codebook, which limits its performance and increases the algorithm's time complexity as the number of discretized rotations grows. Majcher et al. \cite{majcher20203d} also utilize particle filtering in their tracking framework. In their approach, a U-Net is used to segment the target object in advance, and the object's 6-DoF pose in the current frame is estimated by matching the segmented object with a rendered image based on the pose from the last frame. The final pose is obtained by optimizing the difference between object silhouettes and edges, with the particle filter estimating the posterior probability distribution. Zhong et al. \cite{zhong2020seeing} introduced a method that integrates a learning-based video segmentation module into an optimization-based pose estimation pipeline. Wang et al. \cite{wang2023deep} introduced a three-phase pipeline for object pose tracking. The process begins with an FPN-Lite network integrated with MobileNetV2 to extract multi-level features from the current image and to project a 3D object model for deriving 2D contours based on the pose from the previous frame. Next, a boundary prediction network is employed, which takes features from local regions around the contours as input and outputs a probability distribution representing the true boundary locations. Finally, the 6-DoF object pose is refined using Newton's method, leveraging the boundary probability distribution. The primary advantage of tracking by optimization methods lies in their interpretability. However, the optimization process is typically slow, and challenges such as occlusion remain problematic for many methods in this category.

\subsection{Transformer-based Methods}

The Transformer model \cite{vaswani2017attention} has become a dominant architecture for a wide range of tasks, initially in natural language processing (NLP) and more recently in computer vision. Unlike convolutional neural networks (CNNs), which rely on local receptive fields and inductive biases, the Transformer's self-attention mechanism allows it to capture long-range dependencies by attending to different parts of an input sequence. This flexibility has proven to be advantageous in many vision tasks, including image classification, object detection, and pose estimation. In object pose estimation and tracking, transformers have been successfully applied in several recent works \cite{zhou2023deep, hong2024transformer, li2023depth, periyasamy2023yolopose}. Zhou et al. \cite{zhou2023deep} introduced the Deep Fusion Transformer (DFTr) block, designed to aggregate cross-modality features for improved pose estimation. The DFTr captures semantic correlations across modalities by leveraging their similarity, enabling better integration of globally enhanced features for more effective information extraction. Implemented with a transformer-based architecture, the DFTr block captures long-range dependencies, which the authors used to enhance the model's global representation. Similarly, Hong et al. \cite{hong2024transformer} introduced a Transformer-based multi-modal fusion network to enhance object pose estimation performance. In their method, a Vision Transformer (ViT) is used to optimize the feature extraction networks of Convolutional Neural Networks (CNN) and Point Cloud Networks (PCN), establishing a connection between pixel-wise features and global features. While both methods \cite{zhou2023deep, hong2024transformer} rely on RGB and depth images, Li et al. \cite{li2023depth} developed SwinDePose, which uses only depth image geometry for accurate 6D pose estimation. SwinDePose calculates the angles between normal vectors in the depth image and the camera's coordinate axes. These angles are then formed into an image and encoded using the Swin Transformer \cite{liu2021swin}. In contrast, Periyasamy et al. proposed YOLOPose, a Transformer-based method for multi-object 6D pose estimation using only RGB images. YOLOPose extracts features from an RGB image using ResNet, adds positional encoding, and passes them through a six-layer Transformer encoder and decoder. The model generates N embeddings, outputting bounding boxes, class probabilities, translations, and keypoints.

While existing transformer-based methods demonstrate promising results, their performance often deteriorates in scenarios involving occlusions, motion blur, and rapid hand movements, which complicate pose estimation. To address these challenges, we propose a novel transformer-based neural network that explicitly incorporates object visibility and motion information to leverage neighboring frames for predicting the poses of occluded objects. To the best of our knowledge, this is the first study to introduce a visibility-aware module specifically for object pose estimation. Although visibility information has been utilized in related tasks such as scene reconstruction \cite{gao2023visfusion} and multi-view stereo \cite{huang2025visibility, chen2020visibility}, our approach differs significantly in its design and application. Unlike existing visibility-aware mechanisms, which primarily focus on static scene modeling, our module dynamically estimates and adjusts pose predictions in real-time based on an object's visibility in each frame. This is achieved by generating a visibility score through a series of fully connected layers and employing it to identify frames with low visibility. The module then aggregates pose information from more visible frames using a Pose Transformer, which leverages cross-attention mechanisms to fuse temporal information from both occluded and visible frames. This dynamic aggregation ensures accurate pose estimation even under severe occlusions, enabling robust performance in highly dynamic and challenging environments.
