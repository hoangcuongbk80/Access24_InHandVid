\section{Introduction}
\label{sec:intro}

Accurate estimation of an object's pose from images is a crucial task with broad applications in engineering, including robotic assembly \DIFdelbegin \DIFdel{\cite{trabelsi2021pose, hoang2023grasp, wang2021gdr, hoang2022context}, manipulation \cite{wang2019densefusion, hoang2024collision, zakharov2019dpod, tan2024attention} }\DIFdelend \DIFaddbegin \DIFadd{, manipulation }\DIFaddend augmented reality-driven design\DIFdelbegin \DIFdel{\cite{billings2019silhonet, hoang2024object, peng2019pvnet, hoang2025attention}}\DIFdelend , and automated quality inspection systems \DIFdelbegin \DIFdel{\cite{marullo20236d, du2021vision, cho2024integration}}\DIFdelend \DIFaddbegin \DIFadd{\cite{trabelsi2021pose, hoang2024object, wang2021gdr}}\DIFaddend . The pose, comprising both rotation and translation, provides essential information about the object's position and orientation relative to the camera, enabling systems to interact with the environment in a meaningful way \DIFdelbegin \DIFdel{\cite{hoang2024graspability, peng2019pvnet, wang2021gdr, hoang2022voting}}\DIFdelend \DIFaddbegin \DIFadd{\cite{hoang2024graspability, peng2019pvnet, wang2021gdr}}\DIFaddend . However, achieving robust and precise pose estimation, especially under challenging conditions such as occlusions and complex backgrounds, remains an open research problem\DIFdelbegin \DIFdel{\cite{marullo20236d, vu2024occlusion, chen2016innovative}}\DIFdelend . 

In-hand object pose estimation, in particular, presents a unique set of challenges. Unlike static or freely moving objects, in-hand objects are often subjected to rapid and complex motions, as well as frequent occlusions caused by the hand itself or other environmental factors \cite{chao2021dexycb, hoang2024multi, garcia2018first, llop2022benchmarking}. These conditions lead to significant variations in the object's appearance from frame to frame, complicating the task of accurately tracking its pose over time. Moreover, the proximity of the object to the camera often results in partial or full occlusions, making it difficult to extract reliable visual cues for pose estimation \DIFdelbegin \DIFdel{\cite{hoang2016sub, wang20216d, li2019cdpn, hoang2020panoptic}}\DIFdelend \DIFaddbegin \DIFadd{\cite{wang20216d, li2019cdpn, hoang2020panoptic}}\DIFaddend . Traditional approaches to pose estimation have largely relied on either single-frame methods or handcrafted features, which often struggle to capture the intricate spatial and temporal dependencies present in dynamic scenes \cite{billings2019silhonet, peng2019pvnet}. These methods are particularly vulnerable to errors when the object is partially or fully occluded, as they lack the necessary context to accurately infer the object's pose in such scenarios \DIFdelbegin \DIFdel{\cite{hoang2019object, trabelsi2021pose, rad2017bb8, hoang2020object}}\DIFdelend \DIFaddbegin \DIFadd{\cite{trabelsi2021pose, rad2017bb8, hoang2020object}}\DIFaddend . Additionally, the occlusions and hand-object interactions inherent to in-hand pose estimation introduce further complexity, demanding more sophisticated models that can leverage temporal information to compensate for the lack of visual evidence in individual frames.

Another research direction emphasizes joint estimation of both hand and object poses. Many of these approaches, such as \DIFdelbegin \DIFdel{\cite{doosti2020hope, lin2023harmonious, wang2023interacting, woo2023survey}}\DIFdelend \DIFaddbegin \DIFadd{\cite{doosti2020hope, lin2023harmonious, wang2023interacting}}\DIFaddend , rely on RGB inputs and utilize parameterized models like MANO \cite{romero2022embodied} model to capture hand articulation. Estimating both hand and object poses presents significant challenges due to the high flexibility of the hand, frequent self-occlusions, and the complex interactions between the hand and the object. These difficulties often result in increased errors, particularly when precise object localization is essential. Additionally, the hand-object interaction introduces complex, non-rigid transformations that are difficult to model effectively. Despite these challenges, joint estimation of hand and object poses is critical in applications like telemedicine and immersive gaming, where synchronized and accurate hand-object interaction is vital. However, in many practical scenarios, such as smart manufacturing \DIFdelbegin \DIFdel{(\cite{son2022past}) }\DIFdelend or augmented reality for design evaluation\DIFdelbegin \DIFdel{\cite{park2015spatial}}\DIFdelend , the primary concern is accurately tracking the object's pose. For example, in package sorting or AR-based product identification, the main focus is on determining the precise orientation and position of the object, which is crucial for tasks like scanning barcodes or overlaying virtual information. In industrial automation, especially in precision machining or robotic assembly, tracking the object's pose often takes precedence over the hand or tool's pose. For instance, in a high-precision CNC machining setup, the focus is on accurately positioning and orienting the workpiece to ensure that the machining process meets tight tolerances. In these cases, simplifying the problem by concentrating solely on object pose estimation results in more efficient and practical solutions, while the hand's pose becomes secondary to the overall task.

In this paper, we propose a novel approach for in-hand object pose estimation in videos, addressing the inherent challenges posed by dynamic hand movements and frequent occlusions. Our method begins by leveraging a pre-trained convolutional neural network (CNN) to extract rich spatial features from each frame of the video sequence. These features serve as the foundational representation of the object, capturing crucial spatial information that is essential for accurate pose estimation. However, single-frame spatial features alone are insufficient for robust pose tracking in complex, real-world scenarios where objects are often partially or fully occluded by the hand or other environmental elements. To overcome this limitation, we incorporate a transformer-based temporal reasoning module that processes the sequence of frames as a coherent temporal entity. The transformers in our model are specifically designed to capture both spatial and temporal dependencies across frames, allowing the network to model the evolution of the object's pose over time. By attending to relevant contextual information from both past and future frames, the transformer network enhances the precision of pose estimation, even when critical visual cues are momentarily absent due to occlusions. This temporal reasoning capability is particularly vital in in-hand scenarios, where the object's appearance can change rapidly and unpredictably, necessitating a model that can seamlessly integrate information across time. To further enhance the robustness of our method, we introduce a visibility-aware mechanism that dynamically adjusts the pose predictions based on the current visibility of the object in each frame. This mechanism is crucial for handling the occlusions that are common in in-hand object manipulation tasks. As the hand interacts with the object, portions of the object may become obscured, leading to incomplete or ambiguous visual information. Our visibility-aware mechanism mitigates this issue by assessing the visibility of the object in real-time and incorporating evidence from neighboring frames where the object is more visible. By doing so, the mechanism ensures that the pose estimation remains accurate even when direct visual evidence is lacking. \\

The key contributions of our work include:

\begin{itemize}

\item We propose a hybrid architecture that uniquely combines convolutional neural networks (CNNs) with transformers, specifically tailored for in-hand 6D object pose estimation. Unlike conventional methods that simply stack CNNs and transformers, our approach deeply integrates these components, enabling the spatial transformer to effectively capture non-local spatial interactions within each frame, and the temporal transformer to robustly model dependencies across entire video sequences. 

\item We introduce a visibility-aware module that employs a visibility estimation technique to dynamically adjust pose predictions based on the object's visibility in each frame. By utilizing a visibility score generated through a series of fully connected layers, our method identifies frames with low visibility and compensates by aggregating pose information from multiple visible frames using a Pose Transformer. This approach ensures accurate pose estimation even under heavy occlusion, leveraging cross-attention mechanisms to fuse information from occluded and visible frames, thereby maintaining robust performance in challenging scenarios.

\item Comprehensive experimental evaluations demonstrate the robustness and effectiveness of our approach across multiple challenging scenarios, including severe occlusions, fast hand movements, and complex backgrounds. Our method consistently outperforms existing state-of-the-art techniques in both quantitative and qualitative measures, proving its suitability for real-world applications.

\end{itemize}

The remainder of this paper is organized as follows: Section 2 provides an overview of the related work in the field of object pose estimation, discussing methods for single-image pose estimation, deep learning-based object pose tracking, and the emerging use of transformer-based approaches. In Section 3, we detail our proposed method, starting with the hybrid architecture of CNNs for spatial feature extraction and transformers for temporal reasoning. This section includes a comprehensive explanation of the spatial-temporal transformer, covering both the CNN backbone and the spatial and temporal transformers. We also introduce our novel visibility-aware object pose estimation under occlusions and describe the loss functions used to train the model. Section 4 evaluates the effectiveness of our approach, presenting the datasets used, implementation details, evaluation metrics, and experimental results. Additionally, we perform an ablation study to analyze the contributions of each model component. Finally, Section 5 concludes the paper by summarizing our findings and discussing potential future work, particularly the application of our approach to more complex environments and real-time interactions.