\section{Evaluation}
\label{sec:Evaluation}

In this section, we provide a detailed overview of the three publicly available datasets utilized in our experiments: DexYCB \cite{chao2021dexycb}, FPHAB \cite{garcia2018first}, and HO-3D \cite{hampali2020honnotate}. Each of these datasets presents unique challenges and characteristics that are essential for evaluating the robustness and accuracy of in-hand object pose tracking methods. We benchmark our approach against state-of-the-art methods across various metrics, including Average Precision (AP) and Area Under the Curve (AUC), showing that our model consistently achieves higher accuracy, particularly in challenging scenarios involving heavy occlusions and rapid hand movements.

\subsection{Datasets}

\textbf{DexYCB Dataset} \cite{chao2021dexycb}. The DexYCB dataset is highly suitable for evaluating hand-held object pose tracking in videos due to its comprehensive set of annotated hand-object interactions. The dataset includes over 582 video sequences that capture the dynamic manipulation of 20 different objects from the YCB dataset by 10 subjects. Each sequence provides 6D object poses, 3D hand poses, and RGB-D data, which are crucial for testing the robustness and accuracy of tracking algorithms in real-world conditions. The variety of objects, coupled with the naturalistic hand movements, makes DexYCB an ideal benchmark for assessing how well a model can maintain accurate pose estimates over time, particularly in scenarios involving rapid hand movements and frequent occlusions. 

\noindent \textbf{FPHAB Dataset} \cite{garcia2018first}. The First-Person Hand Action Benchmark (FPHAB) dataset is particularly relevant for evaluating pose tracking in first-person view scenarios. It offers 1175 RGB-D video sequences across 45 different action classes, all annotated with 3D hand poses. This dataset is valuable for testing hand-held object pose tracking models because the egocentric viewpoint introduces challenges such as motion blur, self-occlusion, and varying lighting conditions, which are common in real-world applications. By providing a diverse set of actions and interactions, FPHAB enables the assessment of a model's ability to track objects consistently despite the inherent complexities of first-person perspectives. 

\noindent \textbf{HO-3D Dataset} \cite{hampali2020honnotate}. This dataset is specifically designed to evaluate 3D hand-object pose estimation, making it an excellent benchmark for hand-held object pose tracking in videos. With over 80,000 annotated frames, this dataset captures intricate hand-object interactions, often under challenging conditions like severe occlusions and complex background scenes. The sequences in HO-3D are recorded in real-world environments, providing a realistic testbed for assessing the effectiveness of pose tracking algorithms. The dataset's emphasis on naturalistic hand movements and object manipulation in cluttered environments is critical for evaluating how well a model can maintain accurate pose estimates across frames when faced with partial or full occlusions.

\subsection{Implementation Details}

Our implementation\footnote{Our code and other materials are available at \url{https://github.com/hoangcuongbk80/6dInHandVid}} employs ResNet-50 \cite{he2016deep} combined with ROIAlign \cite{he2017mask} to extract object features at an output resolution of $32 \times 32$ with 256 feature channels. The spatial transformer module, which includes both the encoder and decoder, is built with 3 layers of multi-head self-attention, each layer containing 8 attention heads and a feed-forward network dimension of 256. In the transformer decoder, the query vectors are also set to a dimension of 256. The temporal transformer shares the same architecture as the spatial transformer. While the ResNet-50 backbone is initialized with weights pre-trained on ImageNet, the transformer components are trained from scratch using an end-to-end approach. We utilize the Adam optimizer with an initial learning rate of $1 \times 10^{-5}$. Following the adversarial training protocol, the transformers are alternately updated at each training step. Training is conducted over 120 epochs, with the learning rate being reduced by 30\% every 20 epochs. The model is trained across 8 NVIDIA RTX 2080Ti GPUs. For inference, the trained model is deployed on a single RTX 2080Ti GPU.

%-------------------------------------------------------
\begin{figure*}[!ht]
    \begin{subfigure}{0.98\textwidth}
        \includegraphics[width=\linewidth]{figs/1_rgb}
        \caption{Input}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.98\textwidth}
        \includegraphics[width=\linewidth]{figs/1_0}
        \caption{Ground truth}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.98\textwidth}
        \includegraphics[width=\linewidth]{figs/1_1}
        \caption{Ours}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.98\textwidth}
        \includegraphics[width=\linewidth]{figs/1_2}
        \caption{Wang et al. \cite{wang2023deep}}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.98\textwidth}
        \includegraphics[width=\linewidth]{figs/1_3}
        \caption{Castro et al. \cite{castro2023crt}}
    \end{subfigure}
    \caption{Qualitative Comparison of the state-of-the-art single-view method \cite{castro2023crt}, video-based method \cite{wang2023deep},
and our method. The proposed method robustly generates more stable and accurate poses over time than previous approaches. (a) are the input RGBD images. (b) shows the rendered images using ground truth object poses. (c), (d), and (e) display the rendered images using predicted object poses.}
    \label{fig:result1}
\end{figure*}

%-------------------------------------------------------
\begin{figure*}[!ht]
    \begin{subfigure}{0.98\textwidth}
        \includegraphics[width=\linewidth]{figs/2_rgb}
        \caption{Input}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.98\textwidth}
        \includegraphics[width=\linewidth]{figs/2_0}
        \caption{Ground truth}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.98\textwidth}
        \includegraphics[width=\linewidth]{figs/2_1}
        \caption{Ours}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.98\textwidth}
        \includegraphics[width=\linewidth]{figs/2_2}
        \caption{Wang et al. \cite{wang2023deep}}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.98\textwidth}
        \includegraphics[width=\linewidth]{figs/2_3}
        \caption{Castro et al. \cite{castro2023crt}}
    \end{subfigure}
    \caption{Qualitative Comparison of the state-of-the-art single-view method \cite{castro2023crt}, video-based method \cite{wang2023deep},
and our method. Here we only show results on frames under heavy occlusion (frames with a visibility score \( v_i \) smaller than \( \delta = 0.5 \)). (a) are the input RGBD images. (b) shows the rendered images using ground truth hand and object poses. (c), (d), and (e) display the rendered images using ground truth hand poses and predicted object poses.}
    \label{fig:result2}
\end{figure*}
%-------------------------------------------------------

\subsection{Evaluation Metric}
\label{sec:metric}

To evaluate the accuracy of the estimated pose \(\mathbf{\hat{P}}\) compared to the ground-truth pose \(\mathbf{\bar{P}}\) of an object model \(M\), we employ Average Distance of Model Points (ADD) metric \cite{hinterstoisser2012model}. This metric determines the average distance between corresponding vertices of the object in its ground-truth and estimated poses. Mathematically, given the ground-truth rotation \(\mathbf{\bar{R}}\) and translation \(\mathbf{\bar{t}}\), and the estimated rotation \(\mathbf{\hat{R}}\) and translation \(\mathbf{\hat{t}}\), the ADD is expressed as:

\begin{equation}
ADD = \frac{1}{m} \sum_{x \in M} \parallel (\mathbf{\bar{R}}x + \mathbf{\bar{t}}) - (\mathbf{\hat{R}}x + \mathbf{\hat{t}}) \parallel,
\end{equation}

\noindent where \(m\) represents the number of points on the object model. For symmetric objects, the ADD metric is modified to use the minimum distance between corresponding points, as described in \cite{bregier2017symmetry}. We assess the accuracy of the pose predictions using the Average Precision (AP) metric \cite{bregier2017symmetry}, where a 6D pose estimate is classified as a true positive if the ADD is within 10\% of the object's smallest bounding sphere diameter. Additionally, we report the Area Under the ADD Curve (AUC) \cite{wang2019densefusion}, which summarizes the model's performance across a range of error thresholds.

\subsection{Result}

Figures \ref{fig:result1} and \ref{fig:result2} provide a qualitative comparison of the proposed method with existing state-of-the-art methods for in-hand object pose estimation. In Figure \ref{fig:result1}, the proposed method consistently produces more stable and accurate pose predictions over time compared to single-view and video-based methods. In Figure \ref{fig:result2}, the comparison focuses on frames with heavy occlusion. The visibility-aware module in our approach allows the system to leverage temporal information from neighboring frames, resulting in accurate pose estimation despite the lack of clear visual cues. Both Wang et al. and Castro et al.'s methods struggle in these scenarios, as evidenced by the distorted and less accurate object poses. Our method, by contrast, maintains reliable predictions under occlusion, demonstrating the effectiveness of integrating the visibility-aware module into the spatial-temporal architecture.

%----------------------------------------------------------
\begin{table*}[h]
\caption{Quantitative results on the DexYCB \cite{chao2021dexycb}, FPHAB \cite{garcia2018first}, and HO-3D \cite{hampali2020honnotate} datasets. Single image -based methods \cite{billings2019silhonet, peng2019pvnet, wang2021gdr, castro2023crt}, and tracking methods \cite{sun2021robust, huang2021pixel, he2021ffb6d, stoiber2020sparse, stoiber2022srt3d, tian2022large, wang2023deep} are compared with our proposed method (Ours).}
\label{tab:dataset_ex}
\begin{center}
\begin{tabular}{l c c c c c c c} 
\hline
& \multicolumn{2}{c}{DexYCB} & \multicolumn{2}{c}{FPHAB} & \multicolumn{2}{c}{HO-3D} & Time \\
\hline
Method & $AUC$ & $AP$ & $AUC$ & $AP$ & $AUC$ & $AP$ & $ms$ \\  
\hline 
Billings et al. \cite{billings2019silhonet} & 54.1 & 55.2 & 56.5 & 57.4 & 55.7 & 56.2 & 35 \\

Peng et al. \cite{peng2019pvnet} & 57.6 & 58.5 & 59.1 & 60.3 & 58.3 & 59.2 & 38 \\

Wang et al.\cite{wang2021gdr} & 59.3 & 60.8 & 61.2 & 62.6 & 60.4 & 61.4 & 32 \\

Castro et al. \cite{castro2023crt} & 61.5 & 62.4 & 63.6 & 64.3 & 62.6 & 63.7 & 30 \\

\hline 

Sun et al. \cite{sun2021robust} & 60.5 & 61.4 & 63.6 & 64.2 & 61.9 & 62.6 & 87 \\

Huang et al. \cite{huang2021pixel} & 68.3 & 69.5 & 69.5 & 70.2 & 68.1 & 69.4 & 67 \\

Stoiber et al. \cite{stoiber2020sparse} & 67.2 & 67.5 & 68.4 & 69.2 & 67.9 & 68.3 & 98 \\

Stoiber et al. \cite{stoiber2022srt3d} & 71.4 & 72.8 & 73.5 & 74.3 & 71.1 & 72.2 & 87 \\

Tian et al. \cite{tian2022large} & 72.4 & 72.9 & 74.1 & 75.4 & 73.3 & 74.2 & 78 \\

Wang et al. \cite{wang2023deep} & 74.6 & 75.5 & 76.2 & 77.1 & 76.0 & 76.3 & 33 \\

Ours & \textbf{80.1} & \textbf{80.8} & \textbf{81.6} & \textbf{82.6} & \textbf{81.1} & \textbf{82.3} & 66 \\
\hline
\end{tabular}
\end{center}
\end{table*}

%-------------------------------------------------

Table \ref{tab:dataset_ex} presents the quantitative comparison of our proposed method with both single-image-based and tracking-based methods across three datasets: DexYCB \cite{chao2021dexycb}, FPHAB \cite{garcia2018first}, and HO-3D \cite{hampali2020honnotate}. The metrics used for evaluation are AUC (Area Under the ADD Curve) and AP (Average Precision). The table clearly illustrates that single-image-based methods, such as \cite{billings2019silhonet}, \cite{peng2019pvnet}, \cite{wang2021gdr}, and \cite{castro2023crt}, underperform compared to tracking-based methods, which is expected given the importance of temporal information in in-hand object pose tracking tasks. Among the tracking-based methods, \cite{wang2023deep} delivers strong performance with AUC scores of 74.6\%, 76.2\%, and 76.0\% on the DexYCB, FPHAB, and HO-3D datasets, respectively. However, our proposed method significantly outperforms all other approaches, achieving the highest AUC scores of 80.1\%, 81.6\%, and 81.1\% across these datasets. This improvement is attributed to the incorporation of transformers for spatial-temporal modeling and the visibility-aware module, which effectively handles occlusions and maintains robust pose estimation even under challenging conditions. Additionally, our method demonstrates competitive inference time, operating at 66ms per frame. Although not the fastest, it offers a well-balanced trade-off between execution speed and performance, outperforming faster but less accurate methods such as \cite{castro2023crt} and \cite{wang2021gdr}. These results validate the effectiveness of our approach in enhancing the accuracy and robustness of in-hand object pose tracking across diverse real-world scenarios.


%----------------------------------------------------------
\begin{table*}[h]
\caption{Quantitative results on the DexYCB \cite{chao2021dexycb}, FPHAB \cite{garcia2018first}, and HO-3D \cite{hampali2020honnotate} datasets. However, here we only evaluate methods on frames under heavy occlusion (frames with a visibility score \( v_i \) smaller than \( \delta = 0.5 \)). Single image-based methods \cite{billings2019silhonet, peng2019pvnet, wang2021gdr, castro2023crt}, and tracking methods \cite{sun2021robust, huang2021pixel, he2021ffb6d, stoiber2020sparse, stoiber2022srt3d, tian2022large, wang2023deep} are compared with our proposed method (Ours).}
\label{tab:occlusion_ex}
\begin{center}
\begin{tabular}{l c c c c c c c} 
\hline
& \multicolumn{2}{c}{DexYCB } & \multicolumn{2}{c}{FPHAB } & \multicolumn{2}{c}{HO-3D} & Time \\
\hline
Method & $AUC$ & $AP$ & $AUC$ & $AP$ & $AUC$ & $AP$ & $ms$ \\  
\hline 
Billings et al. \cite{billings2019silhonet} & 48.4 & 49.6 & 51.0 & 52.3 & 50.3 & 51.5 & 35 \\

Peng et al. \cite{peng2019pvnet} & 52.1 & 53.6 & 54.5 & 56.1 & 53.0 & 54.1 & 38 \\

Wang et al. \cite{wang2021gdr} & 54.8 & 54.4 & 56.0 & 57.3 & 55.4 & 56.3 & 32 \\

Castro et al. \cite{castro2023crt} & 55.2 & 56.2 & 57.0 & 57.3 & 58.1 & 58.9 & 30 \\

\hline 

Sun et al. \cite{sun2021robust} & 58.2 & 57.1 & 60.1 & 61.2 & 60.2 & 61.3 & 87 \\

Huang et al. \cite{huang2021pixel} & 63.9 & 64.8 & 64.6 & 65.0 & 63.5 & 64.6 & 67 \\

Stoiber et al.\cite{stoiber2020sparse} & 61.1 & 61.5 & 62.6 & 63.1 & 61.4 & 62.0 & 98 \\

Stoiber et al. \cite{stoiber2022srt3d} & 63.5 & 63.9 & 64.6 & 65.1 & 66.7 & 67.0 & 87 \\

Tian et al. \cite{tian2022large} & 65.0 & 65.8 & 67.7 & 68.5 & 66.8 & 67.4 & 78 \\

Wang et al. \cite{wang2023deep} & 67.3 & 68.4 & 69.1 & 70.0 & 69.7 & 69.2 & 33 \\

Ours & \textbf{78.3} & \textbf{78.5} & \textbf{79.4} & \textbf{80.1} & \textbf{79.6} & \textbf{80.8} & 66 \\
\hline
\end{tabular}
\end{center}
\end{table*}
%-------------------------------------------------

Table \ref{tab:occlusion_ex} evaluates the performance of various methods under heavy occlusion, specifically focusing on frames with a visibility score \( v_i \) smaller than \( \delta = 0.5 \). Compared to Table \ref{tab:dataset_ex} in the previous section, which evaluates overall performance, the results here reveal a key strength of our proposed method: its robustness under challenging occlusions. While most methods experience a significant drop in performance when evaluated under heavy occlusion, our method shows only a slight reduction in accuracy. For instance, on the DexYCB dataset, our method achieves an AUC of 78.3\%, compared to 80.1\% in the general evaluation from Table \ref{tab:dataset_ex}, representing a minor 1.8\% decrease. Similarly, on the FPHAB and HO-3D datasets, our method exhibits minimal reductions in AUC, with only a 2.2\% and 1.5\% decrease, respectively. In contrast, other methods suffer more substantial performance degradation under occlusion. For example, \cite{wang2023deep}, which performed well in the general evaluation, drops from 74.6\% to 67.3\% in AUC on the DexYCB dataset, representing a 7.3\% decrease. This trend is consistent across all datasets, where competing methods see larger reductions in both AUC and AP metrics. The relative stability of our method's performance highlights the effectiveness of the proposed modules, particularly the visibility-aware object pose estimation module. By dynamically adjusting pose predictions based on the visibility of the object and aggregating information from less occluded frames, our approach significantly mitigates the impact of occlusions on pose estimation accuracy. This robustness is crucial for maintaining reliable performance in real-world applications where occlusions are common, and further underscores the importance of our hybrid architecture that integrates both spatial-temporal reasoning and visibility-aware mechanisms.

\subsection{Ablation Study}

%----------------------------------------------------------
\begin{table*}[h]
\caption{Ablation Study. $\mathcal{M}_{s}$ spatial transformer, $\mathcal{M}_{t}$ temporal transformer, $\mathcal{M}_{va}$ visibility-aware object pose estimation module.}
\label{tab:ablation_ex}
\begin{center}
\begin{tabular}{l c c c c c c c} 
\hline
& \multicolumn{2}{c}{DexYCB} & \multicolumn{2}{c}{FPHAB} & \multicolumn{2}{c}{HO-3D} & Time \\
\hline
Method & $AUC$ & $AP$ & $AUC$ & $AP$ & $AUC$ & $AP$ & $ms$ \\  
\hline 

Without Transformers & 58.2 & 57.1 & 60.1 & 61.2 & 60.2 & 61.3 & 87 \\

Without $\mathcal{M}_{t}$ and $\mathcal{M}_{s}$ & 65.7 & 66.4 & 68.5 & 68.9 & 67.2 & 68.4 & 28 \\

Without $\mathcal{M}_{s}$ & 73.7 & 74.1 & 74.2 & 75.0 & 76.3 & 77.2 & 35 \\

Without $\mathcal{M}_{t}$ & 71.2 & 72.3 & 74.9 & 76.3 & 73.5 & 74.0 & 36 \\

Without $\mathcal{M}_{va}$ & 75.1 & 77.2 & 76.5 & 76.8 & 75.6 & 76.9 & 34 \\

Ours (Full) & \textbf{80.1} & \textbf{80.8} & \textbf{81.6} & \textbf{82.6} & \textbf{81.1} & \textbf{82.3} & 66 \\
\hline
\end{tabular}
\end{center}
\end{table*}

%-------------------------------------------------

Table \ref{tab:ablation_ex} presents the results of the ablation study, which evaluates the contributions of the spatial transformer module ($\mathcal{M}_{s}$), temporal transformer module ($\mathcal{M}_{t}$), and visibility-aware object pose estimation module ($\mathcal{M}_{va}$) to the overall performance of our proposed method. The goal of this analysis is to assess the impact of each component by systematically removing them from the model and measuring the resulting performance on the DexYCB, FPHAB, and HO-3D datasets.

The results show that removing either the spatial or temporal transformers leads to a significant drop in performance. For instance, when both $\mathcal{M}_{s}$ and $\mathcal{M}_{t}$ are excluded, the AUC drops to 65.7\% on DexYCB, indicating a substantial decrease of 14.4\% compared to the full model. This highlights the importance of spatial-temporal reasoning for capturing the dynamic nature of in-hand object pose tracking. Similarly, removing only the spatial transformer ($\mathcal{M}_{s}$) results in a notable performance drop, with an AUC of 73.7\% on DexYCB, which underscores the critical role that spatial dependencies play in enhancing the model's understanding of object-hand interactions within individual frames. When the temporal transformer ($\mathcal{M}_{t}$) is excluded, the model also suffers a decline in performance across all datasets. For example, on the HO-3D dataset, the AUC decreases from 81.1\% (full model) to 73.5\%, demonstrating the essential role of temporal reasoning in improving pose estimation accuracy across video sequences. This result indicates that while spatial information is crucial, temporal dependencies are equally important for maintaining robust tracking performance over time. The visibility-aware module ($\mathcal{M}_{va}$) also plays a significant role in handling occlusions. Removing this module results in a moderate drop in performance, with the AUC on DexYCB decreasing to 75.1\%. This reduction in accuracy highlights the effectiveness of the visibility-aware mechanism in ensuring reliable pose estimation even in frames where the object is partially or fully occluded. By dynamically adjusting predictions based on object visibility and aggregating information from neighboring frames, this module enhances the model's robustness in challenging scenarios. Overall, the ablation study confirms that the combination of the spatial transformer, temporal transformer, and visibility-aware modules is critical for achieving the best performance. The full model consistently outperforms its ablated variants across all datasets, with an AUC of 80.1\% on DexYCB, 81.6\% on FPHAB, and 81.1\% on HO-3D. Despite the added complexity of integrating these components, the inference time remains competitive at 66ms, demonstrating that the proposed method offers a practical balance between accuracy and efficiency for real-time in-hand object pose tracking applications.

\section{Conclusion}

In this paper, we introduced a novel framework for in-hand object pose tracking that effectively addresses the challenges posed by dynamic occlusions and rapid hand movements. By leveraging a hybrid architecture that combines the strengths of convolutional neural networks (CNNs) for spatial feature extraction with transformers for temporal reasoning, our method achieves robust and accurate pose estimation across video sequences. A key innovation of our approach is the visibility-aware module, which dynamically adjusts pose predictions based on the object's visibility, allowing the model to maintain high accuracy even under challenging conditions where portions of the object are occluded or blurred. Extensive experiments conducted on three public datasets DexYCB, FPHAB, and HO-3D demonstrate the superior performance of our method compared to existing state-of-the-art techniques. Our approach consistently outperforms these methods across various metrics, highlighting its effectiveness in both standard and occlusion-heavy scenarios. The results of the ablation studies further validate the importance of each component in our model, particularly the integration of spatial-temporal transformers and the visibility-aware mechanism, which collectively contribute to the robustness and accuracy of the pose tracking process.

Beyond achieving competitive accuracy, our method also maintains a practical balance between performance and computational efficiency, making it suitable for real-time applications in fields such as robotics, augmented reality, and human-computer interaction. The ability to accurately track objects in-hand, even under challenging conditions, opens up new possibilities for enhancing the interactivity and functionality of systems in these domains. For future work, we plan to explore the extension of our framework to handle more complex and cluttered environments, as well as its application to multi-object pose tracking scenarios. Additionally, we aim to optimize the computational aspects of our model further, enabling faster inference while preserving accuracy, thereby enhancing its applicability to real-time, resource-constrained environments. By continuing to refine and expand our approach, we hope to contribute to the development of more intelligent and adaptive systems capable of understanding and interacting with the physical world in a more sophisticated manner. \\

