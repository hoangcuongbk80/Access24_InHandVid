\relax 
\citation{trabelsi2021pose,hoang2023grasp,wang2021gdr,hoang2022context}
\citation{wang2019densefusion,hoang2024collision,zakharov2019dpod,tan2024attention}
\citation{billings2019silhonet,hoang2024object,peng2019pvnet,hoang2025attention}
\citation{marullo20236d,du2021vision,cho2024integration}
\citation{hoang2024graspability,peng2019pvnet,wang2021gdr,hoang2022voting}
\citation{marullo20236d,vu2024occlusion,chen2016innovative}
\citation{chao2021dexycb,hoang2024multi,garcia2018first,llop2022benchmarking}
\citation{hoang2016sub,wang20216d,li2019cdpn,hoang2020panoptic}
\citation{billings2019silhonet,peng2019pvnet}
\citation{hoang2019object,trabelsi2021pose,rad2017bb8,hoang2020object}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\newlabel{sec:intro}{{I}{1}}
\citation{doosti2020hope,lin2023harmonious,wang2023interacting,woo2023survey}
\citation{romero2022embodied}
\citation{son2022past}
\citation{park2015spatial}
\citation{wang20216d,gao20206d,guo2021efficient}
\citation{billings2019silhonet,peng2019pvnet,wang2021gdr}
\citation{wang2019densefusion,he2020pvn3d,hong2024rdpn6d}
\citation{lowe1999object,lepetit2005monocular}
\citation{xiang2017posecnn,kehl2017ssd,trabelsi2021pose,wang2021gdr}
\citation{rad2017bb8,tekin2018real,oberweger2018making,peng2019pvnet}
\citation{li2019cdpn,park2019pix2pose,zakharov2019dpod}
\citation{xiang2017posecnn,kehl2017ssd,trabelsi2021pose}
\citation{hu2020single,chen2020end,wang2021gdr}
\citation{lepetit2009ep}
\citation{rad2017bb8,tekin2018real,oberweger2018making,peng2019pvnet}
\citation{lepetit2009ep}
\citation{li2019cdpn,park2019pix2pose,zakharov2019dpod}
\citation{sun2021robust,huang2021pixel,stoiber2020sparse,tian2022large}
\citation{garon2017deep,marougkas2020track,manhardt2018deep,li2018deepim,dosovitskiy2015flownet,wen2020se}
\citation{deng2021poserbpf,majcher20203d,zhong2020seeing,wang2023deep}
\citation{garon2017deep}
\citation{marougkas2020track}
\citation{manhardt2018deep}
\citation{li2018deepim}
\citation{dosovitskiy2015flownet}
\citation{wen2020se}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work}{3}}
\newlabel{sec:Related}{{II}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-A}}Object Pose Estimation from a Single Image}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-B}}Deep Learning-based Object Pose Tracking}{3}}
\citation{deng2021poserbpf}
\citation{majcher20203d}
\citation{zhong2020seeing}
\citation{wang2023deep}
\citation{vaswani2017attention}
\citation{yu2023cattrack}
\citation{zhou2023deep,hong2024transformer,li2023depth,periyasamy2023yolopose}
\citation{zhou2023deep}
\citation{amini2022yolopose}
\citation{wen2024foundationpose}
\citation{periyasamy2024motpose}
\citation{hong2024transformer}
\citation{zhou2023deep,hong2024transformer}
\citation{li2023depth}
\citation{li2024efficient}
\citation{liu2021swin}
\citation{li2021pose}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-C}}Transformer-based Methods}{4}}
\citation{gao2023visfusion}
\citation{huang2025visibility,chen2020visibility}
\citation{carion2020end}
\citation{lin2017feature}
\citation{he2016deep}
\citation{vaswani2017attention}
\citation{oberweger2018making,tekin2018real,rad2017bb8}
\citation{jaegle2021perceiver}
\@writefile{toc}{\contentsline {section}{\numberline {III}Method}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Spatial-Temporal Transformer}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-A}1}CNN Backbone}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-A}2}Spatial Transformer}{5}}
\citation{vaswani2017attention}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Overview of the proposed framework for in-hand object pose tracking. Our approach leverages transformers to capture both spatial relationships within individual frames and temporal dependencies across image sequences, enabling robust frame-wise object pose estimation. To address challenges posed by heavy occlusion or motion blur, a visibility-aware module integrates temporally-aware features and aggregates pose information from neighboring frames. This allows the model to maintain accurate pose estimates even in difficult scenarios where direct visual cues are limited or absent.\relax }}{6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:overview}{{1}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-A}3}Temporal Transformer}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}\DIFdelbegin  {\color {red} \relax \fontsize  {7}{8pt}\selectfont  Visibility-aware }\DIFdelend  \DIFaddbegin  {\color {blue} \sf  Visibility-Aware }\DIFaddend  Object Pose Estimation \DIFdelbegin  {\color {red} \relax \fontsize  {7}{8pt}\selectfont  under occlusions}\DIFdelend  \DIFaddbegin  {\color {blue} \sf  Under Occlusions}\DIFaddend  }{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-B}1}Visibility Estimation\DIFdelbegin  {\color {red} \relax \fontsize  {7}{8pt}\selectfont  .}\DIFdelend  }{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-B}2}Object Pose Prediction Under Heavy Occlusion\DIFdelbegin  {\color {red} \relax \fontsize  {7}{8pt}\selectfont  .}\DIFdelend  }{7}}
\citation{chao2021dexycb}
\citation{garcia2018first}
\citation{hampali2020honnotate}
\citation{chao2021dexycb}
\citation{chao2021dexycb}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-C}}Loss Function}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Evaluation}{8}}
\newlabel{sec:Evaluation}{{IV}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Datasets}{8}}
\citation{garcia2018first}
\citation{hampali2020honnotate}
\citation{he2016deep}
\citation{he2017mask}
\citation{wang2023deep}
\citation{wang2023deep}
\citation{wang2023deep}
\citation{castro2023crt}
\citation{wang2023deep}
\citation{castro2023crt}
\citation{castro2023crt}
\citation{wang2023deep}
\citation{wang2023deep}
\citation{wang2023deep}
\citation{wang2023deep}
\citation{castro2023crt}
\citation{wang2023deep}
\citation{castro2023crt}
\citation{castro2023crt}
\citation{wang2023deep}
\citation{hinterstoisser2012model}
\citation{bregier2017symmetry}
\citation{bregier2017symmetry}
\citation{wang2019densefusion}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Implementation Details}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-C}}Evaluation Metric}{9}}
\newlabel{sec:metric}{{\unhbox \voidb@x \hbox {IV-C}}{9}}
\citation{wang2023deep}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \DIFdelbeginFL  {\color {red} \relax \fontsize  {7}{8pt}\selectfont  Castro et al}\DIFdelendFL  \DIFaddbeginFL  {\color {blue} \sf  Qualitative Comparison of the state-of-the-art video-based method \cite  {wang2023deep} and our method. The proposed method robustly generates more stable and accurate poses over time than previous approaches. (a) are the input RGB images. (b-c) shows the rendered images using predicted object poses and colorcoded visualization of point-wise distance error}\DIFaddendFL  .\DIFdelbeginFL  {\color {red} \relax \fontsize  {7}{8pt}\selectfont  \cite  {castro2023crt}}\DIFdelendFL  \relax }}{10}}
\newlabel{fig:result1}{{2}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-D}}Result}{10}}
\citation{chao2021dexycb}
\citation{garcia2018first}
\citation{hampali2020honnotate}
\citation{billings2019silhonet,peng2019pvnet,wang2021gdr,castro2023crt}
\citation{sun2021robust,huang2021pixel,he2021ffb6d,stoiber2020sparse,stoiber2022srt3d,tian2022large,wang2023deep}
\citation{chao2021dexycb}
\citation{garcia2018first}
\citation{hampali2020honnotate}
\citation{billings2019silhonet,peng2019pvnet,wang2021gdr,castro2023crt}
\citation{sun2021robust,huang2021pixel,he2021ffb6d,stoiber2020sparse,stoiber2022srt3d,tian2022large,wang2023deep}
\citation{billings2019silhonet}
\citation{peng2019pvnet}
\citation{wang2021gdr}
\citation{castro2023crt}
\citation{sun2021robust}
\citation{huang2021pixel}
\citation{stoiber2020sparse}
\citation{stoiber2022srt3d}
\citation{tian2022large}
\citation{wang2023deep}
\citation{chao2021dexycb}
\citation{garcia2018first}
\citation{hampali2020honnotate}
\citation{billings2019silhonet}
\citation{peng2019pvnet}
\citation{wang2021gdr}
\citation{castro2023crt}
\citation{wang2023deep}
\citation{castro2023crt}
\citation{wang2021gdr}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \DIFdelbeginFL  {\color {red} \relax \fontsize  {7}{8pt}\selectfont  Castro et al}\DIFdelendFL  \DIFaddbeginFL  {\color {blue} \sf  Qualitative Comparison of the state-of-the-art video-based method \cite  {wang2023deep}, and our method. Here we only show results on frames under heavy occlusion (frames with a visibility score \( v_i \) smaller than \( \delta = 0.5 \)). (a) are the input RGB images. (b-c) shows the rendered images using predicted object poses and colorcoded visualization of point-wise distance error}\DIFaddendFL  .\DIFdelbeginFL  {\color {red} \relax \fontsize  {7}{8pt}\selectfont  \cite  {castro2023crt}}\DIFdelendFL  \relax }}{11}}
\newlabel{fig:result2}{{3}{11}}
\citation{chao2021dexycb}
\citation{garcia2018first}
\citation{hampali2020honnotate}
\citation{billings2019silhonet,peng2019pvnet,wang2021gdr,castro2023crt}
\citation{sun2021robust,huang2021pixel,he2021ffb6d,stoiber2020sparse,stoiber2022srt3d,tian2022large,wang2023deep}
\citation{chao2021dexycb}
\citation{garcia2018first}
\citation{hampali2020honnotate}
\citation{billings2019silhonet,peng2019pvnet,wang2021gdr,castro2023crt}
\citation{sun2021robust,huang2021pixel,he2021ffb6d,stoiber2020sparse,stoiber2022srt3d,tian2022large,wang2023deep}
\citation{billings2019silhonet}
\citation{peng2019pvnet}
\citation{wang2021gdr}
\citation{castro2023crt}
\citation{sun2021robust}
\citation{huang2021pixel}
\citation{stoiber2020sparse}
\citation{stoiber2022srt3d}
\citation{tian2022large}
\citation{wang2023deep}
\citation{wang2023deep}
\citation{zhou2023deep}
\citation{zhou2023deep}
\citation{labbe2020cosypose}
\citation{hong2024transformer}
\citation{hong2024transformer}
\citation{labbe2020cosypose}
\citation{li2023depth}
\citation{li2023depth}
\citation{labbe2020cosypose}
\citation{periyasamy2023yolopose}
\citation{periyasamy2023yolopose}
\citation{labbe2020cosypose}
\citation{labbe2020cosypose}
\citation{zhou2023deep}
\citation{hong2024transformer}
\citation{li2023depth}
\citation{periyasamy2023yolopose}
\citation{labbe2020cosypose}
\citation{zhou2023deep}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Quantitative results on the DexYCB \cite  {chao2021dexycb}, FPHAB \cite  {garcia2018first}, and HO-3D \cite  {hampali2020honnotate} datasets. Single image -based methods \cite  {billings2019silhonet, peng2019pvnet, wang2021gdr, castro2023crt}, and tracking methods \cite  {sun2021robust, huang2021pixel, he2021ffb6d, stoiber2020sparse, stoiber2022srt3d, tian2022large, wang2023deep} are compared with our proposed method (Ours).\relax }}{12}}
\newlabel{tab:dataset_ex}{{1}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-E}}{\color {blue} \sf  Comparison with Transformer-Based Methods}}{12}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Quantitative results on the DexYCB \cite  {chao2021dexycb}, FPHAB \cite  {garcia2018first}, and HO-3D \cite  {hampali2020honnotate} datasets. However, here we only evaluate methods on frames under heavy occlusion (frames with a visibility score \( v_i \) smaller than \( \delta = 0.5 \)). Single image-based methods \cite  {billings2019silhonet, peng2019pvnet, wang2021gdr, castro2023crt}, and tracking methods \cite  {sun2021robust, huang2021pixel, he2021ffb6d, stoiber2020sparse, stoiber2022srt3d, tian2022large, wang2023deep} are compared with our proposed method (Ours).\relax }}{13}}
\newlabel{tab:occlusion_ex}{{2}{13}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces {\color {blue} \sf  Performance comparison of our method with existing transformer-based approaches for 6D object pose estimation across three benchmark datasets. We only evaluate methods on frames under heavy occlusion (frames with a visibility score \( v_i \) smaller than \( \delta = 0.5 \)).}\relax }}{13}}
\newlabel{tab:Transformer-Based}{{3}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-F}}Ablation Study}{13}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Ablation Study. $\mathcal  {M}_{s}$ spatial transformer, $\mathcal  {M}_{t}$ temporal transformer, $\mathcal  {M}_{va}$ visibility-aware object pose estimation module.\relax }}{14}}
\newlabel{tab:ablation_ex}{{4}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-G}}{\color {blue} \sf  Runtime Analysis and Computational Efficiency}}{14}}
\bibstyle{IEEEtran}
\bibdata{References}
\bibcite{trabelsi2021pose}{1}
\bibcite{hoang2023grasp}{2}
\bibcite{wang2021gdr}{3}
\bibcite{hoang2022context}{4}
\bibcite{wang2019densefusion}{5}
\bibcite{hoang2024collision}{6}
\bibcite{zakharov2019dpod}{7}
\bibcite{tan2024attention}{8}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-H}}{\color {blue} \sf  Limitations and Future Work}}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Conclusion}{15}}
\@writefile{toc}{\contentsline {section}{REFERENCES}{15}}
\bibcite{billings2019silhonet}{9}
\bibcite{hoang2024object}{10}
\bibcite{peng2019pvnet}{11}
\bibcite{hoang2025attention}{12}
\bibcite{marullo20236d}{13}
\bibcite{du2021vision}{14}
\bibcite{cho2024integration}{15}
\bibcite{hoang2024graspability}{16}
\bibcite{hoang2022voting}{17}
\bibcite{vu2024occlusion}{18}
\bibcite{chen2016innovative}{19}
\bibcite{chao2021dexycb}{20}
\bibcite{hoang2024multi}{21}
\bibcite{garcia2018first}{22}
\bibcite{llop2022benchmarking}{23}
\bibcite{hoang2016sub}{24}
\bibcite{wang20216d}{25}
\bibcite{li2019cdpn}{26}
\bibcite{hoang2020panoptic}{27}
\bibcite{hoang2019object}{28}
\bibcite{rad2017bb8}{29}
\bibcite{hoang2020object}{30}
\bibcite{doosti2020hope}{31}
\bibcite{lin2023harmonious}{32}
\bibcite{wang2023interacting}{33}
\bibcite{woo2023survey}{34}
\bibcite{romero2022embodied}{35}
\bibcite{son2022past}{36}
\bibcite{park2015spatial}{37}
\bibcite{gao20206d}{38}
\bibcite{guo2021efficient}{39}
\bibcite{he2020pvn3d}{40}
\bibcite{hong2024rdpn6d}{41}
\bibcite{lowe1999object}{42}
\bibcite{lepetit2005monocular}{43}
\bibcite{xiang2017posecnn}{44}
\bibcite{kehl2017ssd}{45}
\bibcite{tekin2018real}{46}
\bibcite{oberweger2018making}{47}
\bibcite{park2019pix2pose}{48}
\bibcite{hu2020single}{49}
\bibcite{chen2020end}{50}
\bibcite{lepetit2009ep}{51}
\bibcite{sun2021robust}{52}
\bibcite{huang2021pixel}{53}
\bibcite{stoiber2020sparse}{54}
\bibcite{tian2022large}{55}
\bibcite{garon2017deep}{56}
\bibcite{marougkas2020track}{57}
\bibcite{manhardt2018deep}{58}
\bibcite{li2018deepim}{59}
\bibcite{dosovitskiy2015flownet}{60}
\bibcite{wen2020se}{61}
\bibcite{deng2021poserbpf}{62}
\bibcite{majcher20203d}{63}
\bibcite{zhong2020seeing}{64}
\bibcite{wang2023deep}{65}
\bibcite{vaswani2017attention}{66}
\bibcite{yu2023cattrack}{67}
\bibcite{zhou2023deep}{68}
\bibcite{hong2024transformer}{69}
\bibcite{li2023depth}{70}
\bibcite{periyasamy2023yolopose}{71}
\bibcite{amini2022yolopose}{72}
\bibcite{wen2024foundationpose}{73}
\bibcite{periyasamy2024motpose}{74}
\bibcite{li2024efficient}{75}
\bibcite{liu2021swin}{76}
\bibcite{li2021pose}{77}
\bibcite{gao2023visfusion}{78}
\bibcite{huang2025visibility}{79}
\bibcite{chen2020visibility}{80}
\bibcite{carion2020end}{81}
\bibcite{lin2017feature}{82}
\bibcite{he2016deep}{83}
\bibcite{jaegle2021perceiver}{84}
\bibcite{hampali2020honnotate}{85}
\bibcite{he2017mask}{86}
\bibcite{castro2023crt}{87}
\bibcite{hinterstoisser2012model}{88}
\bibcite{bregier2017symmetry}{89}
\bibcite{he2021ffb6d}{90}
\bibcite{stoiber2022srt3d}{91}
\bibcite{labbe2020cosypose}{92}
\@writefile{toc}{\contentsline {subsection}{Phan Xuan Tan}{18}}
\@writefile{toc}{\contentsline {subsection}{Dinh-Cuong Hoang}{18}}
\@writefile{toc}{\contentsline {subsection}{Eiji Kamioka}{18}}
\@writefile{toc}{\contentsline {subsection}{Anh-Nhat Nguyen}{18}}
\@writefile{toc}{\contentsline {subsection}{Duc-Thanh Tran}{18}}
\@writefile{toc}{\contentsline {subsection}{Van-Hiep Duong}{18}}
\@writefile{toc}{\contentsline {subsection}{Anh-Truong Mai}{18}}
\@writefile{toc}{\contentsline {subsection}{Duc-Long Pham}{18}}
\@writefile{toc}{\contentsline {subsection}{Khanh-Toan Phan}{18}}
\@writefile{toc}{\contentsline {subsection}{Van-Hiep Duong}{18}}
\@writefile{toc}{\contentsline {subsection}{Xuan-Tung Dinh}{19}}
\@writefile{toc}{\contentsline {subsection}{TranThiThuyTrang}{19}}
\@writefile{toc}{\contentsline {subsection}{Xuan-Duong Pham}{19}}
\@writefile{toc}{\contentsline {subsection}{Nhat-Linh Nguyen}{19}}
\@writefile{toc}{\contentsline {subsection}{{\color {blue} \sf  Thu-Uyen Nguyen}}{19}}
\@writefile{toc}{\contentsline {subsection}{Viet-Anh Trinh}{19}}
\@writefile{toc}{\contentsline {subsection}{Khanh-Duong Tran}{19}}
\@writefile{toc}{\contentsline {subsection}{Son-Anh Bui}{19}}
